"""
VOCè‡ªåŠ¨æ‰“æ ‡ç­¾ç³»ç»Ÿ - ä¸»ç¨‹åº v3.0
"""
import os
import sys
import argparse
import pandas as pd
import shutil
import time
import json
import platform
from pathlib import Path
from datetime import datetime
from tqdm import tqdm
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.metrics import classification_report, confusion_matrix
import numpy as np
import warnings
import logging
warnings.filterwarnings('ignore')

# è·å–é¡¹ç›®æ ¹ç›®å½•
ROOT_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
DATA_DIR = os.path.join(ROOT_DIR, "data")

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
try:
    from roberta_model import VOCMultiTaskClassifier
    from text_processor import TextProcessor
    from data_loader import DataLoader
    from visualizer import VOCVisualizer
    print("âœ… æ–°æ¨¡å—åŠ è½½æˆåŠŸ")
except ImportError as e:
    # å…¼å®¹æ€§å¤„ç†ï¼šå¦‚æœæ–°æ¨¡å—ä¸å­˜åœ¨ï¼Œå°è¯•å¯¼å…¥æ—§ç‰ˆæœ¬
    print(f"âš ï¸ å¯¼å…¥æ–°æ¨¡å—å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨å…¼å®¹æ¨¡å¼: {e}")
    try:
        from roberta_model import RobertaClassifier as VOCMultiTaskClassifier
        from text_processor import TextProcessor
        from data_loader import DataLoader
        from visualizer import Visualizer as VOCVisualizer
        print("âœ… å…¼å®¹æ¨¡å¼åŠ è½½æˆåŠŸ")
    except ImportError as e2:
        print(f"âŒ æ¨¡å—å¯¼å…¥å¤±è´¥: {e2}")
        sys.exit(1)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(os.path.join(DATA_DIR, 'logs', 'main.log')),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# é¢œè‰²è¾“å‡ºå·¥å…·ç±»
class ColorOutput:
    """å½©è‰²è¾“å‡ºå·¥å…·ç±»ï¼Œå…¼å®¹ä¸åŒç»ˆç«¯"""
    RESET = '\033[0m'
    BOLD = '\033[1m'
    RED = '\033[91m'
    GREEN = '\033[92m'
    YELLOW = '\033[93m'
    BLUE = '\033[94m'
    MAGENTA = '\033[95m'
    CYAN = '\033[96m'
    WHITE = '\033[97m'
    
    @staticmethod
    def is_color_supported():
        """æ£€æµ‹ç»ˆç«¯æ˜¯å¦æ”¯æŒå½©è‰²è¾“å‡º"""
        if platform.system() == 'Windows':
            return int(platform.version().split('.')[0]) >= 10
        return True
    
    @classmethod
    def print(cls, text, color=None, bold=False):
        """æ‰“å°å½©è‰²æ–‡æœ¬"""
        if not cls.is_color_supported():
            print(text)
            return
            
        style = ''
        if color:
            style += color
        if bold:
            style += cls.BOLD
            
        if style:
            print(f"{style}{text}{cls.RESET}")
        else:
            print(text)
    
    @classmethod
    def success(cls, text):
        cls.print(f"âœ… {text}", cls.GREEN)
    
    @classmethod
    def error(cls, text):
        cls.print(f"âŒ {text}", cls.RED)
    
    @classmethod
    def warning(cls, text):
        cls.print(f"âš ï¸ {text}", cls.YELLOW)
    
    @classmethod
    def info(cls, text):
        cls.print(f"â„¹ï¸ {text}", cls.CYAN)
    
    @classmethod
    def title(cls, text):
        cls.print(f"\n{'='*60}", cls.CYAN, bold=True)
        cls.print(f"{text.center(60)}", cls.CYAN, bold=True)
        cls.print(f"{'='*60}", cls.CYAN, bold=True)

class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å™¨"""
    def __init__(self):
        self.start_time = None
        self.end_time = None
        self.checkpoints = {}
        
    def start(self, name: str = "main"):
        self.start_time = time.time()
        self.checkpoints[name] = {'start': self.start_time}
        
    def checkpoint(self, name: str):
        current_time = time.time()
        if name not in self.checkpoints:
            self.checkpoints[name] = {}
        self.checkpoints[name]['end'] = current_time
        if self.start_time:
            self.checkpoints[name]['duration'] = current_time - self.start_time
        
    def end(self):
        self.end_time = time.time()
        return self.get_duration()
        
    def get_duration(self):
        if self.start_time and self.end_time:
            return self.end_time - self.start_time
        return 0
        
    def format_duration(self, duration=None):
        if duration is None:
            duration = self.get_duration()
        
        if duration < 60:
            return f"{duration:.2f}ç§’"
        elif duration < 3600:
            return f"{duration//60:.0f}åˆ†{duration%60:.0f}ç§’"
        else:
            hours = duration // 3600
            minutes = (duration % 3600) // 60
            return f"{hours:.0f}å°æ—¶{minutes:.0f}åˆ†"
    
    def get_report(self):
        """è·å–æ€§èƒ½æŠ¥å‘Š"""
        report = []
        for name, times in self.checkpoints.items():
            if 'duration' in times:
                report.append(f"  {name}: {self.format_duration(times['duration'])}")
        return "\n".join(report)

def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='VOCè‡ªåŠ¨æ‰“æ ‡ç­¾ç³»ç»Ÿ v3.0')
    
    # é€šç”¨å‚æ•°
    parser.add_argument('--mode', type=str, 
                        choices=['train', 'predict', 'batch', 'interactive', 'initialize', 'evaluate', 'demo'], 
                        default='interactive', help='è¿è¡Œæ¨¡å¼')
    parser.add_argument('--model_dir', type=str, default=os.path.join(DATA_DIR, 'models/latest'),
                        help='æ¨¡å‹ä¿å­˜/åŠ è½½ç›®å½•')
    parser.add_argument('--config_file', type=str, help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--log_level', type=str, default='INFO', 
                        choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'],
                        help='æ—¥å¿—çº§åˆ«')
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument('--train_file', type=str, help='è®­ç»ƒæ•°æ®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--text_column', type=str, default='text', help='æ–‡æœ¬åˆ—å')
    parser.add_argument('--test_size', type=float, default=0.2, help='æµ‹è¯•é›†æ¯”ä¾‹')
    parser.add_argument('--validation_split', type=float, default=0.2, help='éªŒè¯é›†æ¯”ä¾‹')
    parser.add_argument('--cross_validation', action='store_true', help='æ˜¯å¦æ‰§è¡Œäº¤å‰éªŒè¯')
    parser.add_argument('--use_ensemble', action='store_true', help='æ˜¯å¦ä½¿ç”¨é›†æˆæ–¹æ³•')
    parser.add_argument('--max_length', type=int, default=256, help='æœ€å¤§åºåˆ—é•¿åº¦')
    parser.add_argument('--pooling_strategy', type=str, default='mean', 
                        choices=['cls', 'mean', 'max', 'attention'], help='ç‰¹å¾æå–ç­–ç•¥')
    parser.add_argument('--enable_text_segmentation', action='store_true', help='æ˜¯å¦å¯ç”¨æ–‡æœ¬åˆ†è¯')
    
    # é¢„æµ‹å‚æ•°
    parser.add_argument('--input_file', type=str, help='è¾“å…¥æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output_file', type=str, help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--batch_size', type=int, default=16, help='æ‰¹å¤„ç†å¤§å°')
    parser.add_argument('--threshold', type=float, default=0.3, help='å¤šæ ‡ç­¾åˆ†ç±»é˜ˆå€¼')
    parser.add_argument('--generate_report', action='store_true', help='æ˜¯å¦ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š')
    parser.add_argument('--enable_cache', action='store_true', help='æ˜¯å¦å¯ç”¨æ•°æ®ç¼“å­˜')
    
    return parser.parse_args()

def load_config(config_file):
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    if config_file and os.path.exists(config_file):
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
                logger.info(f"é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ: {config_file}")
                return config
        except Exception as e:
            logger.error(f"é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {str(e)}")
    return {}

def enhanced_train_model(args, interactive_mode=False):
    """å¢å¼ºç‰ˆæ¨¡å‹è®­ç»ƒ"""
    ColorOutput.title("VOCæ™ºèƒ½æ¨¡å‹è®­ç»ƒç³»ç»Ÿ")
    monitor = PerformanceMonitor()
    monitor.start("training")
    
    try:
        # åŠ è½½é…ç½®
        config = load_config(args.config_file) if args.config_file else {}
        
        # åˆå§‹åŒ–ç»„ä»¶
        ColorOutput.info("åˆå§‹åŒ–ç³»ç»Ÿç»„ä»¶...")
        monitor.checkpoint("init_components")
        
        # æ•°æ®åŠ è½½å™¨ - ä½¿ç”¨æ–°ç‰ˆæœ¬
        data_loader = DataLoader(
            cache_dir=os.path.join(DATA_DIR, 'cache'),
            enable_cache=getattr(args, 'enable_cache', True)
        )
        
        # æ–‡æœ¬å¤„ç†å™¨ - ä½¿ç”¨æ–°ç‰ˆæœ¬
        text_processor = TextProcessor(
            enable_userdict=True,
            stopwords_path=config.get('stopwords_path')
        )
        
        # åŠ è½½è®­ç»ƒæ•°æ®
        ColorOutput.info("åŠ è½½è®­ç»ƒæ•°æ®...")
        monitor.checkpoint("load_data")
        
        if args.train_file and os.path.exists(args.train_file):
            texts, labels_dict, df = data_loader.load_file(
                args.train_file, 
                text_column=args.text_column
            )
        else:
            ColorOutput.info("ä½¿ç”¨VOCç¤ºä¾‹æ•°æ®è¿›è¡Œè®­ç»ƒ")
            texts, labels_dict = data_loader.load_examples("voc")
        
        if not texts:
            ColorOutput.error("æ²¡æœ‰å¯ç”¨çš„è®­ç»ƒæ•°æ®")
            return None
        
        # æ•°æ®è´¨é‡åˆ†æ
        ColorOutput.info("åˆ†ææ•°æ®è´¨é‡...")
        quality_report = text_processor.analyze_text_quality(texts)
        ColorOutput.info(f"æ•°æ®è´¨é‡æŠ¥å‘Š: æ€»æ–‡æœ¬{quality_report.get('total_texts', 0)}æ¡, "
                        f"æœ‰æ•ˆç‡{100-quality_report.get('empty_rate', 0):.1f}%")
    
        # æ–‡æœ¬é¢„å¤„ç†
        ColorOutput.info("æ‰§è¡Œæ–‡æœ¬é¢„å¤„ç†...")
        monitor.checkpoint("preprocess_text")
        
        processed_texts = text_processor.batch_process(
            texts,
            enable_segmentation=getattr(args, 'enable_text_segmentation', False),
            show_progress=True
        )
        
        # æå–å…³é”®è¯
        if interactive_mode:
            ColorOutput.info("æå–å…³é”®è¯...")
            keywords = text_processor.extract_keywords(processed_texts, top_k=10)
            ColorOutput.info(f"å…³é”®è¯: {', '.join(keywords.keys())}")
        
        # åˆå§‹åŒ–åˆ†ç±»å™¨
        ColorOutput.info("åˆå§‹åŒ–æ™ºèƒ½åˆ†ç±»å™¨...")
        monitor.checkpoint("init_classifier")
        
        classifier_config = {
            'model_name': config.get('model_name', 'hfl/chinese-roberta-wwm-ext'),
            'max_length': args.max_length,
            'use_pooling': args.pooling_strategy,
            'dropout_rate': config.get('dropout_rate', 0.1)
        }
        
        classifier = VOCMultiTaskClassifier(**classifier_config)
        
        # è®­ç»ƒæ¨¡å‹
        ColorOutput.info("å¼€å§‹æ™ºèƒ½è®­ç»ƒ...")
        monitor.checkpoint("model_training")
        
        train_results = classifier.train(
            processed_texts,
            labels_dict,
            validation_split=args.validation_split,
            use_ensemble=args.use_ensemble
        )
        
        # æ˜¾ç¤ºè®­ç»ƒç»“æœ
        ColorOutput.success("æ¨¡å‹è®­ç»ƒå®Œæˆ!")
        for label_type, result in train_results.items():
            if label_type == 'validation':
                continue
            if 'error' not in result:
                if result['type'] == 'multilabel':
                    ColorOutput.info(f"  ğŸ“Š {label_type}: F1-micro={result.get('f1_micro', 0):.3f}, "
                                   f"F1-macro={result.get('f1_macro', 0):.3f}")
                else:
                    ColorOutput.info(f"  ğŸ“Š {label_type}: F1-weighted={result.get('f1_weighted', 0):.3f}")
        
        # äº¤å‰éªŒè¯
        if args.cross_validation or (interactive_mode and 
                                    input("\nğŸ”„ æ˜¯å¦æ‰§è¡Œäº¤å‰éªŒè¯ä»¥è·å¾—æ›´å‡†ç¡®çš„è¯„ä¼°? (y/n): ").lower() == 'y'):
            ColorOutput.info("æ‰§è¡Œ5æŠ˜äº¤å‰éªŒè¯...")
            monitor.checkpoint("cross_validation")
            
            cv_results = perform_cross_validation(
                processed_texts, labels_dict, classifier_config, args.use_ensemble
            )
            
            ColorOutput.success("äº¤å‰éªŒè¯ç»“æœ:")
            for label_type, scores in cv_results.items():
                mean_score = np.mean(scores)
                std_score = np.std(scores)
                ColorOutput.info(f"  ğŸ“ˆ {label_type}: {mean_score:.4f} Â± {std_score:.4f}")
            
            train_results['cross_validation'] = cv_results
        
        # ä¿å­˜æ¨¡å‹
        ColorOutput.info("ä¿å­˜è®­ç»ƒæ¨¡å‹...")
        monitor.checkpoint("save_model")
        
        os.makedirs(args.model_dir, exist_ok=True)
        classifier.save(args.model_dir)
        
        # ä¿å­˜è®­ç»ƒæŠ¥å‘Š
        save_training_report(train_results, args.model_dir, text_processor.get_stats())
        
        # ä¿å­˜é¢„å¤„ç†å™¨é…ç½®
        processor_config = {
            'enable_segmentation': getattr(args, 'enable_text_segmentation', False),
            'stats': text_processor.get_stats()
        }
        
        with open(os.path.join(args.model_dir, 'text_processor_config.json'), 'w', encoding='utf-8') as f:
            json.dump(processor_config, f, ensure_ascii=False, indent=2)
        
        # æ€§èƒ½æ€»ç»“
        duration = monitor.end()
        ColorOutput.success(f"ğŸ‰ è®­ç»ƒå®Œæˆ! æ€»ç”¨æ—¶: {monitor.format_duration(duration)}")
        ColorOutput.success(f"ğŸ“ æ¨¡å‹å·²ä¿å­˜åˆ°: {args.model_dir}")
        
        if interactive_mode:
            print(f"\nâ±ï¸ æ€§èƒ½æŠ¥å‘Š:")
            print(monitor.get_report())
        
        return classifier
        
    except Exception as e:
        ColorOutput.error(f"è®­ç»ƒå¤±è´¥: {str(e)}")
        logger.error(f"è®­ç»ƒå¤±è´¥: {str(e)}", exc_info=True)
        return None

def perform_cross_validation(texts, labels_dict, classifier_config, use_ensemble=True, n_splits=5):
    """æ‰§è¡Œäº¤å‰éªŒè¯"""
    # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ ‡ç­¾ç±»å‹è¿›è¡Œåˆ†å±‚æŠ½æ ·
    first_label_type = list(labels_dict.keys())[0]
    first_labels = labels_dict[first_label_type]
    
    # ä¸ºåˆ†å±‚æŠ½æ ·å‡†å¤‡æ ‡ç­¾
    if isinstance(first_labels[0], list) or ',' in str(first_labels[0]):
        # å¤šæ ‡ç­¾æƒ…å†µï¼šä½¿ç”¨ç¬¬ä¸€ä¸ªæ ‡ç­¾æˆ–ç©ºæ ‡ç­¾
        stratify_labels = []
        for label in first_labels:
            if isinstance(label, list):
                stratify_labels.append(label[0] if label else 'empty')
            else:
                label_list = [l.strip() for l in str(label).split(',') if l.strip()]
                stratify_labels.append(label_list[0] if label_list else 'empty')
    else:
        stratify_labels = first_labels
    
    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)
    cv_scores = {label_type: [] for label_type in labels_dict.keys()}
        
    for fold, (train_idx, val_idx) in enumerate(kf.split(texts, stratify_labels)):
        ColorOutput.info(f"  ğŸ“Š ç¬¬{fold+1}/{n_splits}æŠ˜éªŒè¯ä¸­...")
            
        # å‡†å¤‡æŠ˜å æ•°æ®
        fold_train_texts = [texts[i] for i in train_idx]
        fold_val_texts = [texts[i] for i in val_idx]
        fold_train_labels = {k: [v[i] for i in train_idx] for k, v in labels_dict.items()}
        fold_val_labels = {k: [v[i] for i in val_idx] for k, v in labels_dict.items()}
            
            # è®­ç»ƒæ¨¡å‹
        fold_classifier = VOCMultiTaskClassifier(**classifier_config)
        fold_classifier.train(fold_train_texts, fold_train_labels, validation_split=0, use_ensemble=use_ensemble)
            
        # è¯„ä¼°
        fold_results = fold_classifier.evaluate(fold_val_texts, fold_val_labels)
        
        # è®°å½•åˆ†æ•°
        for label_type, metrics in fold_results.items():
            if 'f1_micro' in metrics:
                cv_scores[label_type].append(metrics['f1_micro'])
            elif 'f1_weighted' in metrics:
                cv_scores[label_type].append(metrics['f1_weighted'])
    
    return cv_scores

def enhanced_predict_batch(args):
    """å¢å¼ºç‰ˆæ‰¹é‡é¢„æµ‹"""
    ColorOutput.title("VOCæ™ºèƒ½æ‰¹é‡åˆ†æç³»ç»Ÿ")
    monitor = PerformanceMonitor()
    monitor.start("prediction")
    
    try:
        # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
        if not args.input_file or not os.path.exists(args.input_file):
            ColorOutput.error("è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨")
            return
        
        # è®¾ç½®è¾“å‡ºæ–‡ä»¶
        if not args.output_file:
            base_name = os.path.splitext(os.path.basename(args.input_file))[0]
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            args.output_file = os.path.join(DATA_DIR, f"{base_name}_result_{timestamp}.csv")
        
        # åˆå§‹åŒ–ç»„ä»¶
        ColorOutput.info("åˆå§‹åŒ–ç³»ç»Ÿç»„ä»¶...")
        monitor.checkpoint("init_components")
        
        data_loader = DataLoader(enable_cache=getattr(args, 'enable_cache', True))
        
        # åŠ è½½æ•°æ®
        ColorOutput.info(f"åŠ è½½æ•°æ®: {args.input_file}")
        monitor.checkpoint("load_data")
        
        texts, _, df = data_loader.load_file(args.input_file, text_column=args.text_column)
        
        if not texts:
            ColorOutput.error("æ²¡æœ‰å¯é¢„æµ‹çš„æ–‡æœ¬")
            return
        
        ColorOutput.info(f"æ•°æ®åŠ è½½å®Œæˆ: {len(texts)}æ¡æ–‡æœ¬")
        
        # åŠ è½½æ¨¡å‹
        ColorOutput.info(f"åŠ è½½æ™ºèƒ½æ¨¡å‹: {args.model_dir}")
        monitor.checkpoint("load_model")
        
        classifier = VOCMultiTaskClassifier.load(args.model_dir)
        
        # åŠ è½½æ–‡æœ¬å¤„ç†å™¨é…ç½®
        processor_config_path = os.path.join(args.model_dir, 'text_processor_config.json')
        processor_config = {}
        if os.path.exists(processor_config_path):
            with open(processor_config_path, 'r', encoding='utf-8') as f:
                processor_config = json.load(f)
        
        # é¢„å¤„ç†æ–‡æœ¬
        ColorOutput.info("æ‰§è¡Œæ™ºèƒ½æ–‡æœ¬é¢„å¤„ç†...")
        monitor.checkpoint("preprocess_text")
        
        text_processor = TextProcessor()
        processed_texts = text_processor.batch_process(
            texts,
            enable_segmentation=processor_config.get('enable_segmentation', False),
            show_progress=True
        )
        
        # é¢„æµ‹
        ColorOutput.info("æ‰§è¡Œæ™ºèƒ½é¢„æµ‹åˆ†æ...")
        monitor.checkpoint("prediction")
        
        results = classifier.predict(
            processed_texts,
            return_confidence=True,
            threshold=args.threshold
        )
        
        # å¤„ç†ç»“æœ
        predictions = results['predictions']
        confidences = results['confidences']
        
        # æ·»åŠ ç»“æœåˆ°DataFrame
        ColorOutput.info("å¤„ç†é¢„æµ‹ç»“æœ...")
        monitor.checkpoint("process_results")
        
        for label_type, preds in predictions.items():
            if label_type in ['æ—…ç¨‹è§¦ç‚¹', 'é—®é¢˜ç±»å‹']:
                # å¤šæ ‡ç­¾ç»“æœï¼šè½¬æ¢ä¸ºå­—ç¬¦ä¸²
                df[label_type] = [', '.join(pred) if pred else '' for pred in preds]
            else:
                # å•æ ‡ç­¾ç»“æœ
                df[label_type] = preds
            
            # æ·»åŠ ç½®ä¿¡åº¦
            df[f"{label_type}_ç½®ä¿¡åº¦"] = [f"{conf:.3f}" for conf in confidences[label_type]]
        
        # åˆ é™¤ä¸éœ€è¦çš„ç»Ÿè®¡åˆ—ï¼ˆåŸå§‹æ–‡æœ¬é•¿åº¦ã€å¤„ç†åæ–‡æœ¬é•¿åº¦ã€å¤„ç†æ—¶é—´ï¼‰
        for col in ['åŸå§‹æ–‡æœ¬é•¿åº¦', 'å¤„ç†åæ–‡æœ¬é•¿åº¦', 'å¤„ç†æ—¶é—´']:
            if col in df.columns:
                df.drop(columns=col, inplace=True)
        
        # ä¿å­˜ç»“æœ
        ColorOutput.info("ä¿å­˜é¢„æµ‹ç»“æœ...")
        monitor.checkpoint("save_results")
        
        df.to_csv(args.output_file, index=False, encoding='utf-8')
        
        # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
        if args.generate_report:
            ColorOutput.info("ç”Ÿæˆåˆ†ææŠ¥å‘Š...")
            monitor.checkpoint("generate_report")
            
            report_dir = os.path.splitext(args.output_file)[0] + "_report"
            generate_comprehensive_report(df, predictions, confidences, report_dir, text_processor.get_stats())
        
        # æ€§èƒ½æ€»ç»“
        duration = monitor.end()
        ColorOutput.success(f"ğŸ‰ æ‰¹é‡åˆ†æå®Œæˆ! æ€»ç”¨æ—¶: {monitor.format_duration(duration)}")
        ColorOutput.success(f"ğŸ“„ ç»“æœå·²ä¿å­˜åˆ°: {args.output_file}")
        
        if args.generate_report:
            ColorOutput.success(f"ğŸ“Š æŠ¥å‘Šå·²ç”Ÿæˆåˆ°: {report_dir}")
        
        print(f"\nâ±ï¸ æ€§èƒ½æŠ¥å‘Š:")
        print(monitor.get_report())
        
        # æ˜¾ç¤ºé¢„æµ‹ç»Ÿè®¡
        show_prediction_statistics(predictions, confidences)
        
    except Exception as e:
        ColorOutput.error(f"æ‰¹é‡é¢„æµ‹å¤±è´¥: {str(e)}")
        logger.error(f"æ‰¹é‡é¢„æµ‹å¤±è´¥: {str(e)}", exc_info=True)

def generate_comprehensive_report(df, predictions, confidences, output_dir, text_stats):
    """ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š"""
    try:
        # åˆ›å»ºå¯è§†åŒ–å™¨
        visualizer = VOCVisualizer()
        
        # ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š
        generated_files = visualizer.generate_report(df, output_dir, "VOCæ™ºèƒ½åˆ†ææŠ¥å‘Š")
        
        # ç”Ÿæˆæ–‡æœ¬å¤„ç†æŠ¥å‘Š
        text_report = {
            "timestamp": datetime.now().isoformat(),
            "text_processing_stats": text_stats,
            "prediction_stats": generate_prediction_statistics(predictions, confidences),
            "data_quality": {
                "total_records": len(df),
                "valid_predictions": sum(1 for pred in predictions.get('æƒ…æ„Ÿ', []) if pred),
                "avg_confidence": {
                    label_type: float(np.mean(confs)) 
                    for label_type, confs in confidences.items()
                }
            }
        }
        
        # ä¿å­˜æ–‡æœ¬æŠ¥å‘Š
        text_report_path = os.path.join(output_dir, "processing_report.json")
        with open(text_report_path, 'w', encoding='utf-8') as f:
            json.dump(text_report, f, ensure_ascii=False, indent=2)
        
        generated_files['text_report'] = text_report_path
        
        ColorOutput.success(f"ç»¼åˆæŠ¥å‘Šç”Ÿæˆå®Œæˆï¼ŒåŒ…å« {len(generated_files)} ä¸ªæ–‡ä»¶")
        
    except Exception as e:
        ColorOutput.error(f"æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}")

def generate_prediction_statistics(predictions, confidences):
    """ç”Ÿæˆé¢„æµ‹ç»Ÿè®¡ä¿¡æ¯"""
    stats = {}
    
    for label_type, preds in predictions.items():
        if label_type in ['æ—…ç¨‹è§¦ç‚¹', 'é—®é¢˜ç±»å‹']:
            # å¤šæ ‡ç­¾ç»Ÿè®¡
            all_labels = []
            for pred in preds:
                if pred:
                    all_labels.extend(pred)
            
            label_counts = {}
            for label in all_labels:
                label_counts[label] = label_counts.get(label, 0) + 1
            
            stats[label_type] = {
                "type": "multilabel",
                "unique_labels": len(set(all_labels)),
                "total_predictions": len(all_labels),
                "avg_labels_per_sample": len(all_labels) / len(preds) if preds else 0,
                "top_labels": dict(sorted(label_counts.items(), key=lambda x: x[1], reverse=True)[:5])
            }
        else:
            # å•æ ‡ç­¾ç»Ÿè®¡
            label_counts = {}
            for pred in preds:
                if pred:
                    label_counts[pred] = label_counts.get(pred, 0) + 1
            
            stats[label_type] = {
                "type": "single",
                "unique_labels": len(set(preds)),
                "distribution": label_counts
            }
        
        # ç½®ä¿¡åº¦ç»Ÿè®¡
        confs = confidences[label_type]
        stats[label_type]["confidence"] = {
            "mean": float(np.mean(confs)),
            "std": float(np.std(confs)),
            "min": float(np.min(confs)),
            "max": float(np.max(confs)),
            "high_confidence_rate": float(np.mean([c > 0.8 for c in confs]))
        }
    
    return stats

def show_prediction_statistics(predictions, confidences):
    """æ˜¾ç¤ºé¢„æµ‹ç»Ÿè®¡ä¿¡æ¯"""
    ColorOutput.title("é¢„æµ‹ç»“æœç»Ÿè®¡")
    
    for label_type, preds in predictions.items():
        print(f"\nğŸ“Š {label_type}:")
        
        if label_type in ['æ—…ç¨‹è§¦ç‚¹', 'é—®é¢˜ç±»å‹']:
            # å¤šæ ‡ç­¾ç»Ÿè®¡
            all_labels = [label for pred_list in preds for label in pred_list if pred_list]
            unique_labels = len(set(all_labels))
            avg_labels = len(all_labels) / len(preds) if preds else 0
            
            print(f"  â€¢ æ€»é¢„æµ‹æ•°: {len(all_labels)}")
            print(f"  â€¢ å”¯ä¸€æ ‡ç­¾æ•°: {unique_labels}")
            print(f"  â€¢ å¹³å‡æ ‡ç­¾æ•°/æ ·æœ¬: {avg_labels:.2f}")
        else:
            # å•æ ‡ç­¾ç»Ÿè®¡
            valid_preds = [pred for pred in preds if pred]
            unique_labels = len(set(valid_preds))
            
            print(f"  â€¢ æœ‰æ•ˆé¢„æµ‹æ•°: {len(valid_preds)}")
            print(f"  â€¢ å”¯ä¸€æ ‡ç­¾æ•°: {unique_labels}")
        
        # ç½®ä¿¡åº¦ç»Ÿè®¡
        confs = confidences[label_type]
        mean_conf = np.mean(confs)
        high_conf_rate = np.mean([c > 0.8 for c in confs]) * 100
        
        print(f"  â€¢ å¹³å‡ç½®ä¿¡åº¦: {mean_conf:.3f}")
        print(f"  â€¢ é«˜ç½®ä¿¡åº¦æ¯”ä¾‹: {high_conf_rate:.1f}%")

def predict_single_enhanced(text, model_dir, threshold=0.3):
    """å¢å¼ºç‰ˆå•æ–‡æœ¬é¢„æµ‹"""
    try:
        # åŠ è½½æ¨¡å‹
        classifier = VOCMultiTaskClassifier.load(model_dir)
        
        # åŠ è½½æ–‡æœ¬å¤„ç†å™¨é…ç½®
        processor_config_path = os.path.join(model_dir, 'text_processor_config.json')
        processor_config = {}
        if os.path.exists(processor_config_path):
            with open(processor_config_path, 'r', encoding='utf-8') as f:
                processor_config = json.load(f)
        
        # é¢„å¤„ç†æ–‡æœ¬
        text_processor = TextProcessor()
        processed_text = text_processor.preprocess(
            text,
            enable_segmentation=processor_config.get('enable_segmentation', False)
        )
        
        # é¢„æµ‹
        result = classifier.predict_single_text(processed_text, format_output=True)
        
        # æ·»åŠ å¤„ç†ä¿¡æ¯
        result['å¤„ç†ä¿¡æ¯'] = {
            'åŸå§‹é•¿åº¦': len(text),
            'å¤„ç†åé•¿åº¦': len(processed_text),
            'é¢„æµ‹æ—¶é—´': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            'é˜ˆå€¼': threshold
        }
        
        return result
    except Exception as e:
        ColorOutput.error(f"é¢„æµ‹å¤±è´¥: {str(e)}")
        return None

def save_training_report(train_results, output_dir, text_stats):
    """ä¿å­˜è®­ç»ƒæŠ¥å‘Š"""
    report = {
        "timestamp": datetime.now().isoformat(),
        "training_results": train_results,
        "text_processing_stats": text_stats,
        "model_info": {
            "framework": "transformers + sklearn",
            "base_model": "hfl/chinese-roberta-wwm-ext",
            "version": "3.0"
        },
        "system_info": {
            "python_version": platform.python_version(),
            "platform": platform.platform()
        }
    }
    
    report_path = os.path.join(output_dir, "training_report.json")
    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    ColorOutput.info(f"è®­ç»ƒæŠ¥å‘Šå·²ä¿å­˜: {report_path}")

def initialize_system_enhanced(model_dir):
    """å¢å¼ºç‰ˆç³»ç»Ÿåˆå§‹åŒ–"""
    ColorOutput.title("VOCç³»ç»Ÿåˆå§‹åŒ–")
    
    try:
        # åˆ›å»ºå¤‡ä»½
        if os.path.exists(model_dir):
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_dir = f"{model_dir}_backup_{timestamp}"
            shutil.copytree(model_dir, backup_dir)
            ColorOutput.info(f"å·²å¤‡ä»½ç°æœ‰æ¨¡å‹åˆ°: {backup_dir}")
        
        # æ¸…é™¤ç°æœ‰æ•°æ®
        if os.path.exists(model_dir):
            shutil.rmtree(model_dir)
            ColorOutput.success("ç°æœ‰æ¨¡å‹æ•°æ®å·²æ¸…é™¤")
        
        # åˆ›å»ºç›®å½•ç»“æ„
        dirs_to_create = [
            model_dir,
            os.path.join(DATA_DIR, 'cache'),
            os.path.join(DATA_DIR, 'logs'),
            os.path.join(DATA_DIR, 'reports'),
            os.path.join(DATA_DIR, 'temp')
        ]
        
        for dir_path in dirs_to_create:
            os.makedirs(dir_path, exist_ok=True)
        
        ColorOutput.success("ç›®å½•ç»“æ„åˆ›å»ºå®Œæˆ")
        
        # æ¸…ç†ç¼“å­˜
        cache_dir = os.path.join(DATA_DIR, 'cache')
        if os.path.exists(cache_dir):
            for file in os.listdir(cache_dir):
                file_path = os.path.join(cache_dir, file)
                if os.path.isfile(file_path):
                    os.remove(file_path)
            ColorOutput.success("ç¼“å­˜æ¸…ç†å®Œæˆ")
    
        # éªŒè¯ç¯å¢ƒ
        try:
            import torch
            from transformers import AutoTokenizer
            
            ColorOutput.info("ç¯å¢ƒéªŒè¯:")
            if torch.cuda.is_available():
                ColorOutput.info(f"  âœ… GPUå¯ç”¨: {torch.cuda.get_device_name()}")
            else:
                ColorOutput.info("  â„¹ï¸ ä½¿ç”¨CPUæ¨¡å¼")
            
            # æµ‹è¯•æ¨¡å‹è®¿é—®
            tokenizer = AutoTokenizer.from_pretrained('hfl/chinese-roberta-wwm-ext')
            ColorOutput.info("  âœ… æ¨¡å‹è®¿é—®æ­£å¸¸")
            
        except Exception as e:
            ColorOutput.warning(f"ç¯å¢ƒéªŒè¯è­¦å‘Š: {str(e)}")
        
        ColorOutput.success("ğŸ‰ ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼")
        return True
        
    except Exception as e:
        ColorOutput.error(f"ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {str(e)}")
        return False

def demo_mode():
    """æ¼”ç¤ºæ¨¡å¼"""
    ColorOutput.title("VOCç³»ç»Ÿæ¼”ç¤ºæ¨¡å¼")
    
    # æ¼”ç¤ºæ•°æ®
    demo_texts = [
        "ç†æƒ³ONEçš„è¯­éŸ³åŠ©æ‰‹è¯†åˆ«å¾ˆå‡†ç¡®ï¼Œä½†æ˜¯å¯¼èˆªå¶å°”ä¼šç»•è·¯",
        "å……ç”µé€Ÿåº¦æ¯”å®£ä¼ çš„æ…¢ï¼Œç»­èˆªä¹Ÿä¸å¦‚é¢„æœŸ",
        "é”€å”®é¡¾é—®æœåŠ¡å¾ˆå¥½ï¼Œäº¤è½¦æµç¨‹å¾ˆé¡ºç•…",
        "åº§æ¤…å¾ˆèˆ’é€‚ï¼Œéš”éŸ³æ•ˆæœä¹Ÿä¸é”™ï¼Œå¼€èµ·æ¥å¾ˆå®‰é™",
        "OTAå‡çº§åç³»ç»Ÿæ›´æµç•…äº†ï¼Œæ–°å¢çš„åŠŸèƒ½ä¹Ÿå¾ˆå®ç”¨"
    ]
    
    model_dir = os.path.join(DATA_DIR, 'models/latest')
    
    # æ£€æŸ¥æ˜¯å¦æœ‰è®­ç»ƒå¥½çš„æ¨¡å‹
    if not os.path.exists(model_dir) or not any(f.endswith('.pkl') for f in os.listdir(model_dir)):
        ColorOutput.warning("æœªæ‰¾åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œå°†ä½¿ç”¨ç¤ºä¾‹æ•°æ®è®­ç»ƒ")
        
        # åˆ›å»ºä¸´æ—¶å‚æ•°å¯¹è±¡
        demo_args = argparse.Namespace(
            train_file=None,
            text_column='text',
            test_size=0.2,
            validation_split=0.2,
            model_dir=model_dir,
            cross_validation=False,
            use_ensemble=True,
            max_length=256,
            pooling_strategy='mean',
            config_file=None,
            enable_text_segmentation=False
        )
        
        # è®­ç»ƒæ¨¡å‹
        classifier = enhanced_train_model(demo_args, interactive_mode=False)
        if not classifier:
            ColorOutput.error("æ¼”ç¤ºæ¨¡å‹è®­ç»ƒå¤±è´¥")
            return
    
    # æ¼”ç¤ºé¢„æµ‹
    ColorOutput.info("\nğŸ¯ å¼€å§‹æ¼”ç¤ºé¢„æµ‹åˆ†æ...")
    
    for i, text in enumerate(demo_texts, 1):
        ColorOutput.info(f"\n--- æ¼”ç¤º {i}/{len(demo_texts)} ---")
        print(f"ğŸ“ åŸå§‹æ–‡æœ¬: {text}")
        
        result = predict_single_enhanced(text, model_dir)
        
        if result:
            print("ğŸ¯ åˆ†æç»“æœ:")
            for key, value in result.items():
                if key == 'ç½®ä¿¡åº¦' or key == 'å¤„ç†ä¿¡æ¯':
                    continue
                
                confidence = result.get('ç½®ä¿¡åº¦', {}).get(key, 0)
                if isinstance(value, list):
                    value_str = ', '.join(value) if value else 'æ— '
                else:
                    value_str = str(value)
                
                print(f"  {key}: {value_str} (ç½®ä¿¡åº¦: {confidence:.3f})")
        
        time.sleep(1)  # æ¼”ç¤ºé—´éš”
    
    ColorOutput.success("ğŸ‰ æ¼”ç¤ºå®Œæˆï¼")

def interactive_mode_enhanced():
    """å¢å¼ºç‰ˆäº¤äº’å¼æ¨¡å¼"""
    ColorOutput.title("VOCæ™ºèƒ½æ ‡ç­¾ç³»ç»Ÿ v3.0")
    
    model_dir = os.path.join(DATA_DIR, 'models/latest')
    model_exists = os.path.exists(model_dir) and any(f.endswith('.pkl') for f in os.listdir(model_dir))
    
    if not model_exists:
        ColorOutput.warning("æœªæ£€æµ‹åˆ°é¢„è®­ç»ƒæ¨¡å‹")
        choice = input("æ˜¯å¦ä½¿ç”¨VOCç¤ºä¾‹æ•°æ®è®­ç»ƒæ¨¡å‹? (y/n): ")
        if choice.lower() == 'y':
            args = argparse.Namespace(
                train_file=None,
                text_column='text',
                test_size=0.2,
                validation_split=0.2,
                model_dir=model_dir,
                cross_validation=False,
                use_ensemble=True,
                max_length=256,
                pooling_strategy='mean',
                config_file=None,
                enable_text_segmentation=False
            )
            enhanced_train_model(args, interactive_mode=True)
        else:
            ColorOutput.warning("è¯·å…ˆè®­ç»ƒæ¨¡å‹")
            return
    
    while True:
        print_enhanced_menu()
        choice = input("\nè¯·é€‰æ‹©åŠŸèƒ½ (1-8): ").strip()
        
        try:
            if choice == '1':
                handle_single_prediction(model_dir)
            elif choice == '2':
                handle_batch_prediction(model_dir)
            elif choice == '3':
                handle_model_training(model_dir)
            elif choice == '4':
                handle_model_evaluation(model_dir)
            elif choice == '5':
                handle_system_initialize(model_dir)
            elif choice == '6':
                handle_system_info()
            elif choice == '7':
                demo_mode()
            elif choice == '8':
                ColorOutput.success("æ„Ÿè°¢ä½¿ç”¨VOCæ™ºèƒ½æ ‡ç­¾ç³»ç»Ÿï¼")
                break
            else:
                ColorOutput.error("æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")
        except KeyboardInterrupt:
            ColorOutput.warning("\næ“ä½œå·²ä¸­æ–­")
        except Exception as e:
            ColorOutput.error(f"æ“ä½œå¤±è´¥: {str(e)}")
            logger.error(f"æ“ä½œå¤±è´¥: {str(e)}", exc_info=True)

def print_enhanced_menu():
    """æ‰“å°å¢å¼ºç‰ˆèœå•ï¼ˆä¸­æ–‡ï¼Œå¸¦emojiï¼‰ï¼Œç¡®ä¿è¾¹æ¡†å¯¹é½ç¾è§‚"""
    from wcwidth import wcswidth

    menu_items = [
        "1. ğŸ’¬ å•æ¡æ–‡æœ¬æ™ºèƒ½åˆ†æ",
        "2. ğŸ“Š æ‰¹é‡CSVæ–‡ä»¶å¤„ç†",
        "3. ğŸ¯ è®­ç»ƒ/æ›´æ–°æ¨¡å‹",
        "4. ğŸ“ˆ æ¨¡å‹æ€§èƒ½è¯„ä¼°",
        "5. ğŸ”„ ç³»ç»Ÿåˆå§‹åŒ–",
        "6. ğŸ§¾ ç³»ç»Ÿä¿¡æ¯",
        "7. ğŸª æ¼”ç¤ºæ¨¡å¼",
        "8. ğŸšª é€€å‡ºç³»ç»Ÿ"
    ]
    title = "VOCæ™ºèƒ½åˆ†æç³»ç»Ÿ v3.0"
    # è®¡ç®—æœ€å¤§æ˜¾ç¤ºå®½åº¦ï¼ˆèœå•é¡¹å’Œæ ‡é¢˜éƒ½è¦è€ƒè™‘ï¼‰
    max_menu_width = max(wcswidth(item) for item in menu_items)
    title_width = wcswidth(title)
    box_width = max(max_menu_width, title_width) + 6  # é€‚å½“åŠ é•¿

    print("\n" + "â”Œ" + "â”€" * box_width + "â”")
    # æ‰‹åŠ¨å±…ä¸­æ ‡é¢˜
    title_pad = box_width - wcswidth(title)
    left_pad = title_pad // 2
    right_pad = title_pad - left_pad
    print("â”‚" + " " * left_pad + title + " " * right_pad + "â”‚")
    print("â”œ" + "â”€" * box_width + "â”¤")
    for item in menu_items:
        pad = box_width - wcswidth(item)
        print("â”‚ " + item + " " * (pad - 1) + "â”‚")
    print("â””" + "â”€" * box_width + "â”˜")

def handle_single_prediction(model_dir):
    """å¤„ç†å•æ–‡æœ¬é¢„æµ‹"""
    ColorOutput.info("\n=== å•æ–‡æœ¬æ™ºèƒ½åˆ†æ ===")
    text = input("è¯·è¾“å…¥è¦åˆ†æçš„æ–‡æœ¬: ").strip()
    
    if not text:
        ColorOutput.error("æ–‡æœ¬ä¸èƒ½ä¸ºç©º")
        return
    
    ColorOutput.info("ğŸ§  æ™ºèƒ½åˆ†æä¸­...")
    result = predict_single_enhanced(text, model_dir)
    
    if result:
        ColorOutput.success("\nğŸ“‹ æ™ºèƒ½åˆ†æç»“æœ:")
        print("=" * 50)
        
        for key, value in result.items():
            if key in ['ç½®ä¿¡åº¦', 'å¤„ç†ä¿¡æ¯']:
                continue
                
            confidence = result.get('ç½®ä¿¡åº¦', {}).get(key, 0)
            if isinstance(value, list):
                value_str = ', '.join(value) if value else 'æ— '
            else:
                value_str = str(value)
            
            print(f"{key:12}: {value_str}")
            print(f"{'ç½®ä¿¡åº¦':12}: {confidence:.3f}")
            print("-" * 50)
        
        # æ˜¾ç¤ºå¤„ç†ä¿¡æ¯
        if 'å¤„ç†ä¿¡æ¯' in result:
            info = result['å¤„ç†ä¿¡æ¯']
            print(f"\nğŸ“Š å¤„ç†ç»Ÿè®¡:")
            print(f"  åŸå§‹é•¿åº¦: {info['åŸå§‹é•¿åº¦']} å­—ç¬¦")
            print(f"  å¤„ç†åé•¿åº¦: {info['å¤„ç†åé•¿åº¦']} å­—ç¬¦")
            print(f"  é¢„æµ‹æ—¶é—´: {info['é¢„æµ‹æ—¶é—´']}")
    else:
        ColorOutput.error("åˆ†æå¤±è´¥ï¼Œè¯·æ£€æŸ¥æ¨¡å‹æˆ–é‡è¯•")

def handle_batch_prediction(model_dir):
    """å¤„ç†æ‰¹é‡é¢„æµ‹"""
    ColorOutput.info("\n=== æ‰¹é‡æ–‡ä»¶å¤„ç† ===")
    
    input_file = input("è¯·è¾“å…¥æ–‡ä»¶è·¯å¾„: ").strip()
    if not input_file or not os.path.exists(input_file):
        ColorOutput.error("æ–‡ä»¶ä¸å­˜åœ¨")
        return
    
    text_column = input("æ–‡æœ¬åˆ—å (é»˜è®¤'text'): ").strip() or "text"
    
    output_file = input("è¾“å‡ºæ–‡ä»¶è·¯å¾„ (å¯é€‰): ").strip()
    if not output_file:
        base_name = os.path.splitext(os.path.basename(input_file))[0]
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = os.path.join(DATA_DIR, f"{base_name}_result_{timestamp}.csv")
    
    threshold = input("å¤šæ ‡ç­¾åˆ†ç±»é˜ˆå€¼ (é»˜è®¤0.3): ").strip()
    try:
        threshold = float(threshold) if threshold else 0.3
    except ValueError:
        threshold = 0.3
    
    generate_report = input("æ˜¯å¦ç”Ÿæˆåˆ†ææŠ¥å‘Š? (y/n): ").strip().lower() == 'y'
    
    # åˆ›å»ºå‚æ•°å¯¹è±¡
    args = argparse.Namespace(
        input_file=input_file,
        text_column=text_column,
        output_file=output_file,
        model_dir=model_dir,
        threshold=threshold,
        generate_report=generate_report,
        enable_cache=True
    )
    
    enhanced_predict_batch(args)

def handle_model_training(model_dir):
    """å¤„ç†æ¨¡å‹è®­ç»ƒ"""
    ColorOutput.info("\n=== æ™ºèƒ½æ¨¡å‹è®­ç»ƒ ===")
    
    train_file = input("è®­ç»ƒæ•°æ®æ–‡ä»¶è·¯å¾„ (ç•™ç©ºä½¿ç”¨VOCç¤ºä¾‹æ•°æ®): ").strip()
    if train_file and not os.path.exists(train_file):
        ColorOutput.error("æ–‡ä»¶ä¸å­˜åœ¨")
        return
    
    text_column = input("æ–‡æœ¬åˆ—å (é»˜è®¤'text'): ").strip() or "text"
    
    # é«˜çº§é€‰é¡¹
    print("\nğŸ”§ é«˜çº§è®­ç»ƒé€‰é¡¹:")
    use_ensemble = input("ä½¿ç”¨é›†æˆå­¦ä¹ æ–¹æ³•? (y/n, é»˜è®¤y): ").strip().lower() != 'n'
    enable_segmentation = input("å¯ç”¨ä¸­æ–‡åˆ†è¯? (y/n, é»˜è®¤n): ").strip().lower() == 'y'
    
    pooling_options = ['cls', 'mean', 'max', 'attention']
    print("ç‰¹å¾æå–ç­–ç•¥:")
    for i, option in enumerate(pooling_options, 1):
        print(f"  {i}. {option}")
    
    pooling_choice = input("é€‰æ‹©ç­–ç•¥ (é»˜è®¤2-mean): ").strip()
    try:
        pooling_idx = int(pooling_choice) - 1
        pooling_strategy = pooling_options[pooling_idx] if 0 <= pooling_idx < len(pooling_options) else 'mean'
    except ValueError:
        pooling_strategy = 'mean'
    
    max_length = input("æœ€å¤§åºåˆ—é•¿åº¦ (é»˜è®¤256): ").strip()
    try:
        max_length = int(max_length) if max_length else 256
    except ValueError:
        max_length = 256
    
    # åˆ›å»ºå‚æ•°å¯¹è±¡
    args = argparse.Namespace(
        train_file=train_file,
        text_column=text_column,
        test_size=0.2,
        validation_split=0.2,
        model_dir=model_dir,
        cross_validation=False,
        use_ensemble=use_ensemble,
        max_length=max_length,
        pooling_strategy=pooling_strategy,
        config_file=None,
        enable_text_segmentation=enable_segmentation
    )
    
    enhanced_train_model(args, interactive_mode=True)
            
def handle_model_evaluation(model_dir):
    """å¤„ç†æ¨¡å‹è¯„ä¼°"""
    ColorOutput.info("\n=== æ¨¡å‹æ€§èƒ½è¯„ä¼° ===")
    
    if not os.path.exists(model_dir):
        ColorOutput.error("æ¨¡å‹ä¸å­˜åœ¨ï¼Œè¯·å…ˆè®­ç»ƒæ¨¡å‹")
        return
    
    test_file = input("æµ‹è¯•æ•°æ®æ–‡ä»¶è·¯å¾„: ").strip()
    if not test_file or not os.path.exists(test_file):
        ColorOutput.error("æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨")
        return
    
    text_column = input("æ–‡æœ¬åˆ—å (é»˜è®¤'text'): ").strip() or "text"
    
    try:
        # åŠ è½½æ¨¡å‹
        ColorOutput.info("åŠ è½½æ¨¡å‹...")
        classifier = VOCMultiTaskClassifier.load(model_dir)
        
        # åŠ è½½æµ‹è¯•æ•°æ®
        data_loader = DataLoader()
        texts, labels_dict, df = data_loader.load_file(test_file, text_column=text_column)
        
        if not texts or not labels_dict:
            ColorOutput.error("æµ‹è¯•æ•°æ®æ ¼å¼ä¸æ­£ç¡®")
            return
        
        # é¢„å¤„ç†
        text_processor = TextProcessor()
        processed_texts = text_processor.batch_process(texts, show_progress=True)
        
        # è¯„ä¼°
        ColorOutput.info("æ‰§è¡Œæ¨¡å‹è¯„ä¼°...")
        eval_results = classifier.evaluate(processed_texts, labels_dict)
        
        # æ˜¾ç¤ºç»“æœ
        ColorOutput.success("\nğŸ“Š æ¨¡å‹è¯„ä¼°ç»“æœ:")
        print("=" * 60)
        for label_type, metrics in eval_results.items():
            print(f"\nğŸ“ˆ {label_type}:")
            for metric, value in metrics.items():
                print(f"  {metric:15}: {value:.4f}")
        
        # ä¿å­˜è¯„ä¼°æŠ¥å‘Š
        report_path = os.path.join(model_dir, "evaluation_report.json")
        eval_report = {
            "timestamp": datetime.now().isoformat(),
            "test_file": test_file,
            "results": eval_results,
            "test_data_stats": {
                "total_samples": len(texts),
                "processed_samples": len(processed_texts),
                "text_processing_stats": text_processor.get_stats()
            }
        }
        
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(eval_report, f, ensure_ascii=False, indent=2)
        
        ColorOutput.success(f"\nğŸ“„ è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        
    except Exception as e:
        ColorOutput.error(f"è¯„ä¼°å¤±è´¥: {str(e)}")

def handle_system_initialize(model_dir):
    """å¤„ç†ç³»ç»Ÿåˆå§‹åŒ–"""
    ColorOutput.warning("\n=== ç³»ç»Ÿåˆå§‹åŒ– ===")
    ColorOutput.warning("âš ï¸ æ­¤æ“ä½œå°†æ¸…é™¤æ‰€æœ‰ç°æœ‰æ¨¡å‹æ•°æ®å’Œç¼“å­˜!")
    
    confirm = input("ç¡®è®¤è¦åˆå§‹åŒ–ç³»ç»Ÿå—? (è¾“å…¥'YES'ç¡®è®¤): ").strip()
    if confirm == 'YES':
        if initialize_system_enhanced(model_dir):
            ColorOutput.success("ç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ!")
        else:
            ColorOutput.error("ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥!")
    else:
        ColorOutput.info("æ“ä½œå·²å–æ¶ˆ")

def handle_system_info():
    """æ˜¾ç¤ºç³»ç»Ÿä¿¡æ¯"""
    ColorOutput.info("\n=== ç³»ç»Ÿä¿¡æ¯ ===")
    
    # ç¯å¢ƒä¿¡æ¯
    import torch
    import transformers
    import sklearn
    
    info = {
        "Pythonç‰ˆæœ¬": platform.python_version(),
        "æ“ä½œç³»ç»Ÿ": f"{platform.system()} {platform.release()}",
        "PyTorchç‰ˆæœ¬": torch.__version__,
        "Transformersç‰ˆæœ¬": transformers.__version__,
        "Scikit-learnç‰ˆæœ¬": sklearn.__version__,
        "CUDAå¯ç”¨": "æ˜¯" if torch.cuda.is_available() else "å¦",
        "ç³»ç»Ÿç‰ˆæœ¬": "v3.0"
    }
    
    if torch.cuda.is_available():
        info["GPUè®¾å¤‡"] = torch.cuda.get_device_name()
        info["GPUå†…å­˜"] = f"{torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB"
    
    print("=" * 50)
    for key, value in info.items():
        print(f"{key:20}: {value}")
    print("=" * 50)
    
    # æ¨¡å‹ä¿¡æ¯
    model_dir = os.path.join(DATA_DIR, 'models/latest')
    if os.path.exists(model_dir):
        config_path = os.path.join(model_dir, 'config.json')
        if os.path.exists(config_path):
            with open(config_path, 'r', encoding='utf-8') as f:
                model_config = json.load(f)
            
            print("\nğŸ¤– å½“å‰æ¨¡å‹é…ç½®:")
            for key, value in model_config.items():
                print(f"{key:20}: {value}")
        
        # æ˜¾ç¤ºæ–‡ä»¶ç»Ÿè®¡
        model_files = [f for f in os.listdir(model_dir) if f.endswith('.pkl')]
        print(f"\nğŸ“ æ¨¡å‹æ–‡ä»¶: {len(model_files)}ä¸ª")
        
        # æ˜¾ç¤ºè®­ç»ƒæŠ¥å‘Š
        report_path = os.path.join(model_dir, 'training_report.json')
        if os.path.exists(report_path):
            with open(report_path, 'r', encoding='utf-8') as f:
                report = json.load(f)
            
            print(f"ğŸ“Š æœ€åè®­ç»ƒæ—¶é—´: {report.get('timestamp', 'æœªçŸ¥')}")
        else:
            ColorOutput.warning("æœªæ‰¾åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹")
    else:
        ColorOutput.warning("æœªæ‰¾åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹")

def main():
    """ä¸»ç¨‹åºå…¥å£"""
    try:
        # åˆ›å»ºå¿…è¦ç›®å½•
        dirs_to_create = [
            DATA_DIR,
            os.path.join(DATA_DIR, 'models'),
            os.path.join(DATA_DIR, 'logs'),
            os.path.join(DATA_DIR, 'reports'),
            os.path.join(DATA_DIR, 'cache'),
            os.path.join(DATA_DIR, 'temp')
        ]
        
        for dir_path in dirs_to_create:
            os.makedirs(dir_path, exist_ok=True)
        
        # è§£æå‚æ•°
        args = parse_args()
        
        # è®¾ç½®æ—¥å¿—çº§åˆ«
        logging.getLogger().setLevel(getattr(logging, args.log_level))
        
        # æ ¹æ®æ¨¡å¼æ‰§è¡Œå¯¹åº”åŠŸèƒ½
        if args.mode == 'train':
            enhanced_train_model(args)
        elif args.mode == 'batch':
            enhanced_predict_batch(args)
        elif args.mode == 'predict':
            # å•æ–‡æœ¬é¢„æµ‹æ¨¡å¼
            if len(sys.argv) > 2:
                text = sys.argv[2]
                result = predict_single_enhanced(text, args.model_dir)
                if result:
                    print(json.dumps(result, ensure_ascii=False, indent=2))
                else:
                    sys.exit(1)
            else:
                ColorOutput.error("predictæ¨¡å¼éœ€è¦æä¾›æ–‡æœ¬å‚æ•°")
        elif args.mode == 'evaluate':
            handle_model_evaluation(args.model_dir)
        elif args.mode == 'initialize':
            handle_system_initialize(args.model_dir)
        elif args.mode == 'demo':
            demo_mode()
        elif args.mode == 'interactive':
            interactive_mode_enhanced()
        else:
            ColorOutput.error(f"æœªæ”¯æŒçš„æ¨¡å¼: {args.mode}")
            
    except KeyboardInterrupt:
        ColorOutput.warning("\nç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        ColorOutput.error(f"ç¨‹åºæ‰§è¡Œå¤±è´¥: {str(e)}")
        logger.error(f"ç¨‹åºæ‰§è¡Œå¤±è´¥: {str(e)}", exc_info=True)

if __name__ == "__main__":
    main() 