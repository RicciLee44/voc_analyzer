"""
æ•°æ®åŠ è½½å’Œå¤„ç†å™¨ - æ”¹è¿›ç‰ˆ
æ”¯æŒå¤šç§æ•°æ®æºã€è‡ªåŠ¨ç¼–ç æ£€æµ‹ã€æ•°æ®éªŒè¯ã€å¢é‡åŠ è½½ç­‰åŠŸèƒ½
"""
import os
import pandas as pd
import numpy as np
import json
import time
import chardet
from pathlib import Path
from typing import List, Dict, Tuple, Optional, Union, Any
from tqdm import tqdm
import logging
from datetime import datetime
import pickle

class DataLoader:
    """
    å¢å¼ºç‰ˆæ•°æ®åŠ è½½å™¨
    æ”¯æŒCSVã€JSONã€Excelç­‰å¤šç§æ ¼å¼ï¼Œè‡ªåŠ¨ç¼–ç æ£€æµ‹ï¼Œæ•°æ®éªŒè¯ç­‰åŠŸèƒ½
    """
    
    def __init__(self, cache_dir: Optional[str] = None, enable_cache: bool = True):
        """
        åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨
        
        Args:
            cache_dir: ç¼“å­˜ç›®å½•
            enable_cache: æ˜¯å¦å¯ç”¨ç¼“å­˜
        """
        self.logger = logging.getLogger(__name__)
        self.enable_cache = enable_cache
        self.cache_dir = cache_dir or "data/cache"
        
        # åˆ›å»ºç¼“å­˜ç›®å½•
        if self.enable_cache:
            os.makedirs(self.cache_dir, exist_ok=True)
        
        # æ”¯æŒçš„æ–‡ä»¶æ ¼å¼
        self.supported_formats = {
            '.csv': self._load_csv_file,
            '.json': self._load_json_file,
            '.jsonl': self._load_jsonl_file,
            '.xlsx': self._load_excel_file,
            '.xls': self._load_excel_file,
            '.tsv': self._load_tsv_file,
            '.txt': self._load_txt_file,
            '.parquet': self._load_parquet_file
        }
        
        # ç†æƒ³æ±½è½¦VOCæ ‡ç­¾ä½“ç³»å®šä¹‰
        self.voc_label_schema = {
            'æ—…ç¨‹è§¦ç‚¹': {
                'type': 'multilabel',
                'values': [
                    'å“ç‰Œè®¤çŸ¥', 'å®˜ç½‘/Appæµè§ˆä½“éªŒ', 'åˆ°åº—å’¨è¯¢/è¯•é©¾é¢„çº¦',
                    'è®¢è´­æµç¨‹', 'é”€å”®æœåŠ¡æ€åº¦', 'ä»·æ ¼é€æ˜åº¦',
                    'äº¤è½¦é€Ÿåº¦ä¸æµç¨‹', 'äº¤ä»˜åŸ¹è®­', 'é—¨åº—æœåŠ¡ç¯å¢ƒ',
                    'åŠ é€Ÿã€åˆ¶åŠ¨ã€æ“æ§', 'æ‚¬æŒ‚ä¸èˆ’é€‚åº¦', 'å™ªéŸ³ä¸éš”éŸ³ä½“éªŒ',
                    'æ™ºèƒ½å¯¼èˆª', 'è¯­éŸ³åŠ©æ‰‹', 'HUD/ä¸­æ§äº¤äº’', 'OTAæ›´æ–°ä½“éªŒ',
                    'å®¶ç”¨å……ç”µæ¡©', 'å…¬å…±å……ç”µç«™ä½“éªŒ', 'ç»­èˆªè¡¨ç°',
                    'ä¿å…»ä¸ç»´ä¿®', 'å®¢æœå“åº”é€Ÿåº¦', 'äºŒæ‰‹è½¦/ç½®æ¢æœåŠ¡'
                ]
            },
            'é—®é¢˜ç±»å‹': {
                'type': 'multilabel',
                'values': [
                    'ç¨³å®šæ€§é—®é¢˜', 'æ€§èƒ½é—®é¢˜', 'å¯ç”¨æ€§é—®é¢˜', 'å…¼å®¹æ€§é—®é¢˜',
                    'ç¾è§‚åº¦é—®é¢˜', 'äº¤äº’é€»è¾‘é—®é¢˜', 'å®‰å…¨éšæ‚£', 
                    'æœåŠ¡ä½“éªŒé—®é¢˜', 'æœŸå¾…è½å·®'
                ]
            },
            'æƒ…æ„Ÿ': {
                'type': 'single',
                'values': ['æ­£é¢', 'ä¸­æ€§', 'è´Ÿé¢']
            }
        }
        
        # æ•°æ®ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'loaded_files': 0,
            'total_records': 0,
            'valid_records': 0,
            'invalid_records': 0,
            'encoding_detections': {},
            'load_times': []
        }
    
    def detect_encoding(self, file_path: str, sample_size: int = 10000) -> str:
        """
        è‡ªåŠ¨æ£€æµ‹æ–‡ä»¶ç¼–ç 
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            sample_size: æ£€æµ‹æ ·æœ¬å¤§å°
            
        Returns:
            æ£€æµ‹åˆ°çš„ç¼–ç 
        """
        try:
            with open(file_path, 'rb') as f:
                raw_data = f.read(sample_size)
                result = chardet.detect(raw_data)
                encoding = result['encoding']
                confidence = result['confidence']
                
                self.logger.info(f"ç¼–ç æ£€æµ‹: {encoding} (ç½®ä¿¡åº¦: {confidence:.2f})")
                self.stats['encoding_detections'][file_path] = {
                    'encoding': encoding,
                    'confidence': confidence
                }
                
                return encoding
        except Exception as e:
            self.logger.warning(f"ç¼–ç æ£€æµ‹å¤±è´¥: {str(e)}")
            return 'utf-8'
    
    def _get_cache_path(self, file_path: str) -> str:
        """è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        file_hash = hash(file_path + str(os.path.getmtime(file_path)))
        cache_name = f"cache_{abs(file_hash)}.pkl"
        return os.path.join(self.cache_dir, cache_name)
    
    def _load_from_cache(self, cache_path: str) -> Optional[Tuple]:
        """ä»ç¼“å­˜åŠ è½½æ•°æ®"""
        if not self.enable_cache or not os.path.exists(cache_path):
            return None
        
        try:
            with open(cache_path, 'rb') as f:
                cached_data = pickle.load(f)
                self.logger.info(f"ä»ç¼“å­˜åŠ è½½æ•°æ®: {cache_path}")
                return cached_data
        except Exception as e:
            self.logger.warning(f"ç¼“å­˜åŠ è½½å¤±è´¥: {str(e)}")
            return None
    
    def _save_to_cache(self, cache_path: str, data: Tuple):
        """ä¿å­˜æ•°æ®åˆ°ç¼“å­˜"""
        if not self.enable_cache:
            return
        
        try:
            with open(cache_path, 'wb') as f:
                pickle.dump(data, f)
                self.logger.info(f"æ•°æ®å·²ç¼“å­˜: {cache_path}")
        except Exception as e:
            self.logger.warning(f"ç¼“å­˜ä¿å­˜å¤±è´¥: {str(e)}")
    
    def _load_csv_file(self, file_path: str, **kwargs) -> pd.DataFrame:
        """åŠ è½½CSVæ–‡ä»¶"""
        encoding = kwargs.get('encoding') or self.detect_encoding(file_path)
        
        # å°è¯•å¤šç§ç¼–ç 
        encodings = [encoding, 'utf-8', 'gbk', 'gb2312', 'utf-8-sig', 'latin1']
        
        for enc in encodings:
            try:
                df = pd.read_csv(file_path, encoding=enc, **{k: v for k, v in kwargs.items() if k != 'encoding'})
                self.logger.info(f"æˆåŠŸä½¿ç”¨ç¼–ç  {enc} åŠ è½½CSVæ–‡ä»¶")
                return df
            except UnicodeDecodeError:
                continue
            except Exception as e:
                self.logger.error(f"åŠ è½½CSVæ–‡ä»¶å¤±è´¥ (ç¼–ç : {enc}): {str(e)}")
                continue
        
        raise ValueError(f"æ— æ³•åŠ è½½CSVæ–‡ä»¶ {file_path}ï¼Œå°è¯•äº†æ‰€æœ‰ç¼–ç ")
    
    def _load_json_file(self, file_path: str, **kwargs) -> pd.DataFrame:
        """åŠ è½½JSONæ–‡ä»¶"""
        encoding = kwargs.get('encoding') or self.detect_encoding(file_path)
        
        try:
            with open(file_path, 'r', encoding=encoding) as f:
                data = json.load(f)
                
            if isinstance(data, list):
                df = pd.DataFrame(data)
            elif isinstance(data, dict):
                df = pd.DataFrame([data])
            else:
                raise ValueError("ä¸æ”¯æŒçš„JSONæ ¼å¼")
                
            return df
        except Exception as e:
            self.logger.error(f"åŠ è½½JSONæ–‡ä»¶å¤±è´¥: {str(e)}")
            raise
    
    def _load_jsonl_file(self, file_path: str, **kwargs) -> pd.DataFrame:
        """åŠ è½½JSONLæ–‡ä»¶ (æ¯è¡Œä¸€ä¸ªJSONå¯¹è±¡)"""
        encoding = kwargs.get('encoding') or self.detect_encoding(file_path)
        
        try:
            data = []
            with open(file_path, 'r', encoding=encoding) as f:
                for line in f:
                    if line.strip():
                        data.append(json.loads(line.strip()))
            
            return pd.DataFrame(data)
        except Exception as e:
            self.logger.error(f"åŠ è½½JSONLæ–‡ä»¶å¤±è´¥: {str(e)}")
            raise
    
    def _load_excel_file(self, file_path: str, **kwargs) -> pd.DataFrame:
        """åŠ è½½Excelæ–‡ä»¶"""
        try:
            # å°è¯•è¯»å–ç¬¬ä¸€ä¸ªå·¥ä½œè¡¨
            df = pd.read_excel(file_path, **kwargs)
            return df
        except Exception as e:
            self.logger.error(f"åŠ è½½Excelæ–‡ä»¶å¤±è´¥: {str(e)}")
            raise
    
    def _load_tsv_file(self, file_path: str, **kwargs) -> pd.DataFrame:
        """åŠ è½½TSVæ–‡ä»¶"""
        kwargs['sep'] = '\t'
        return self._load_csv_file(file_path, **kwargs)
    
    def _load_txt_file(self, file_path: str, **kwargs) -> pd.DataFrame:
        """åŠ è½½çº¯æ–‡æœ¬æ–‡ä»¶ (æ¯è¡Œä½œä¸ºä¸€æ¡è®°å½•)"""
        encoding = kwargs.get('encoding') or self.detect_encoding(file_path)
        text_column = kwargs.get('text_column', 'text')
        
        try:
            with open(file_path, 'r', encoding=encoding) as f:
                lines = [line.strip() for line in f if line.strip()]
            
            return pd.DataFrame({text_column: lines})
        except Exception as e:
            self.logger.error(f"åŠ è½½TXTæ–‡ä»¶å¤±è´¥: {str(e)}")
            raise
    
    def _load_parquet_file(self, file_path: str, **kwargs) -> pd.DataFrame:
        """åŠ è½½Parquetæ–‡ä»¶"""
        try:
            import pyarrow.parquet as pq
            df = pd.read_parquet(file_path, **kwargs)
            return df
        except ImportError:
            self.logger.error("éœ€è¦å®‰è£…pyarrowåº“æ¥è¯»å–Parquetæ–‡ä»¶: pip install pyarrow")
            raise
        except Exception as e:
            self.logger.error(f"åŠ è½½Parquetæ–‡ä»¶å¤±è´¥: {str(e)}")
            raise
    
    def load_file(self, 
                  file_path: str, 
                  text_column: str = "text",
                  encoding: Optional[str] = None,
                  use_cache: bool = True,
                  **kwargs) -> Tuple[List[str], Dict[str, List], pd.DataFrame]:
        """
        é€šç”¨æ–‡ä»¶åŠ è½½æ¥å£
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            text_column: æ–‡æœ¬åˆ—å
            encoding: æŒ‡å®šç¼–ç 
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            (texts, labels_dict, dataframe)
        """
        start_time = time.time()
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        
        # è·å–æ–‡ä»¶æ‰©å±•å
        file_ext = Path(file_path).suffix.lower()
        if file_ext not in self.supported_formats:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_ext}")
        
        # å°è¯•ä»ç¼“å­˜åŠ è½½
        cache_path = self._get_cache_path(file_path)
        if use_cache and self.enable_cache:
            cached_data = self._load_from_cache(cache_path)
            if cached_data:
                load_time = time.time() - start_time
                self.stats['load_times'].append(load_time)
                return cached_data
        
        # æ˜¾ç¤ºåŠ è½½è¿›åº¦
        with tqdm(total=5, desc="æ•°æ®åŠ è½½", ncols=80) as pbar:
            # æ­¥éª¤1: æ£€æµ‹æ–‡ä»¶æ ¼å¼
            pbar.set_description("æ£€æµ‹æ–‡ä»¶æ ¼å¼")
            time.sleep(0.1)
            pbar.update(1)
            
            # æ­¥éª¤2: è¯»å–æ–‡ä»¶
            pbar.set_description("è¯»å–æ–‡ä»¶")
            try:
                loader_func = self.supported_formats[file_ext]
                df = loader_func(file_path, encoding=encoding, **kwargs)
                self.logger.info(f"æˆåŠŸåŠ è½½æ–‡ä»¶: {file_path}")
            except Exception as e:
                self.logger.error(f"æ–‡ä»¶åŠ è½½å¤±è´¥: {str(e)}")
                raise
            pbar.update(1)
            
            # æ­¥éª¤3: éªŒè¯æ•°æ®æ ¼å¼
            pbar.set_description("éªŒè¯æ•°æ®æ ¼å¼")
            self._validate_dataframe(df, text_column)
            pbar.update(1)
            
            # æ­¥éª¤4: æå–æ–‡æœ¬å’Œæ ‡ç­¾
            pbar.set_description("æå–æ–‡æœ¬å’Œæ ‡ç­¾")
            texts, labels_dict = self._extract_texts_and_labels(df, text_column)
            pbar.update(1)
            
            # æ­¥éª¤5: æ•°æ®è´¨é‡æ£€æŸ¥
            pbar.set_description("æ•°æ®è´¨é‡æ£€æŸ¥")
            self._check_data_quality(texts, labels_dict)
            pbar.update(1)
        
        # ä¿å­˜åˆ°ç¼“å­˜
        result = (texts, labels_dict, df)
        if use_cache and self.enable_cache:
            self._save_to_cache(cache_path, result)
        
        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        load_time = time.time() - start_time
        self.stats['loaded_files'] += 1
        self.stats['total_records'] += len(df)
        self.stats['load_times'].append(load_time)
        
        self.logger.info(f"æ•°æ®åŠ è½½å®Œæˆ: {len(texts)}æ¡æ–‡æœ¬, {len(labels_dict)}ç§æ ‡ç­¾, ç”¨æ—¶{load_time:.2f}ç§’")
        
        return result
    
    def load_csv(self, 
                 file_path: str, 
                 text_column: str = "text", 
                 encoding: Optional[str] = None,
                 **kwargs) -> Tuple[List[str], Dict[str, List], pd.DataFrame]:
        """
        åŠ è½½CSVæ–‡ä»¶ (ä¿æŒå‘åå…¼å®¹)
        
        Args:
            file_path: CSVæ–‡ä»¶è·¯å¾„
            text_column: æ–‡æœ¬åˆ—å
            encoding: æ–‡ä»¶ç¼–ç 
            
        Returns:
            (texts, labels_dict, dataframe)
        """
        return self.load_file(file_path, text_column, encoding, **kwargs)
    
    def _validate_dataframe(self, df: pd.DataFrame, text_column: str):
        """éªŒè¯DataFrameæ ¼å¼"""
        if df.empty:
            raise ValueError("æ–‡ä»¶ä¸ºç©º")
        
        if text_column not in df.columns:
            available_cols = df.columns.tolist()
            raise ValueError(f"æœªæ‰¾åˆ°æ–‡æœ¬åˆ— '{text_column}'ã€‚å¯ç”¨åˆ—: {available_cols}")
        
        # æ£€æŸ¥æ–‡æœ¬åˆ—æ˜¯å¦æœ‰æœ‰æ•ˆæ•°æ®
        valid_texts = df[text_column].dropna().astype(str).str.strip()
        valid_count = (valid_texts != '').sum()
        
        if valid_count == 0:
            raise ValueError(f"æ–‡æœ¬åˆ— '{text_column}' æ²¡æœ‰æœ‰æ•ˆæ•°æ®")
        
        self.logger.info(f"æ•°æ®éªŒè¯é€šè¿‡: {len(df)}è¡Œ, {len(df.columns)}åˆ—, {valid_count}æ¡æœ‰æ•ˆæ–‡æœ¬")
    
    def _extract_texts_and_labels(self, df: pd.DataFrame, text_column: str) -> Tuple[List[str], Dict[str, List]]:
        """ä»DataFrameæå–æ–‡æœ¬å’Œæ ‡ç­¾"""
        # æå–æ–‡æœ¬
        texts = df[text_column].fillna("").astype(str).tolist()
        
        # æå–æ ‡ç­¾
        labels_dict = {}
        for col in df.columns:
            if col != text_column:
                labels_dict[col] = df[col].fillna("").astype(str).tolist()
        
        return texts, labels_dict
    
    def _check_data_quality(self, texts: List[str], labels_dict: Dict[str, List]):
        """æ£€æŸ¥æ•°æ®è´¨é‡"""
        total_texts = len(texts)
        
        # æ–‡æœ¬è´¨é‡æ£€æŸ¥
        empty_texts = sum(1 for text in texts if not text.strip())
        short_texts = sum(1 for text in texts if len(text.strip()) < 5)
        long_texts = sum(1 for text in texts if len(text.strip()) > 1000)
        
        self.stats['valid_records'] = total_texts - empty_texts
        self.stats['invalid_records'] = empty_texts
        
        # æ ‡ç­¾è´¨é‡æ£€æŸ¥
        label_stats = {}
        for label_type, labels in labels_dict.items():
            non_empty_labels = sum(1 for label in labels if label.strip())
            unique_labels = len(set(label.strip() for label in labels if label.strip()))
            
            label_stats[label_type] = {
                'total': len(labels),
                'non_empty': non_empty_labels,
                'unique': unique_labels,
                'coverage': non_empty_labels / len(labels) * 100 if labels else 0
            }
        
        # æ‰“å°è´¨é‡æŠ¥å‘Š
        print(f"\nğŸ“Š æ•°æ®è´¨é‡æŠ¥å‘Š:")
        print(f"  æ€»æ–‡æœ¬æ•°: {total_texts}")
        print(f"  æœ‰æ•ˆæ–‡æœ¬: {total_texts - empty_texts} ({(total_texts - empty_texts) / total_texts * 100:.1f}%)")
        print(f"  ç©ºæ–‡æœ¬: {empty_texts}")
        print(f"  çŸ­æ–‡æœ¬(<5å­—ç¬¦): {short_texts}")
        print(f"  é•¿æ–‡æœ¬(>1000å­—ç¬¦): {long_texts}")
        
        if label_stats:
            print(f"\nğŸ·ï¸ æ ‡ç­¾è´¨é‡:")
            for label_type, stats in label_stats.items():
                print(f"  {label_type}:")
                print(f"    è¦†ç›–ç‡: {stats['coverage']:.1f}%")
                print(f"    å”¯ä¸€å€¼: {stats['unique']}")
    
    def load_examples(self, example_type: str = "default") -> Tuple[List[str], Dict[str, List]]:
        """
        åŠ è½½ç¤ºä¾‹æ•°æ®
        
        Args:
            example_type: ç¤ºä¾‹ç±»å‹ ('default', 'voc', 'multilabel')
            
        Returns:
            (texts, labels_dict)
        """
        print(f"ğŸ”„ åŠ è½½ç¤ºä¾‹æ•°æ® ({example_type})...")
        
        if example_type == "voc":
            # ç†æƒ³æ±½è½¦VOCç¤ºä¾‹æ•°æ®
            examples = [
                {
                    "text": "å¯¼èˆªç»å¸¸å¸¦æˆ‘ç»•è¿œè·¯ï¼Œè¯­éŸ³ä¹Ÿè¯†åˆ«ä¸å‡ºæ¥ï¼Œè¿˜è€æ˜¯é—ªé€€",
                    "æ—…ç¨‹è§¦ç‚¹": ["æ™ºèƒ½å¯¼èˆª", "è¯­éŸ³åŠ©æ‰‹"],
                    "é—®é¢˜ç±»å‹": ["ç¨³å®šæ€§é—®é¢˜", "å¯ç”¨æ€§é—®é¢˜"],
                    "æƒ…æ„Ÿ": "è´Ÿé¢"
                },
                {
                    "text": "äº¤è½¦å½“å¤©å°å“¥è®²è§£å¾—å¾ˆç»†ï¼Œåº§æ¤…æŒ‰æ‘©ä¹Ÿæ¯”æƒ³è±¡ä¸­èˆ’æœ",
                    "æ—…ç¨‹è§¦ç‚¹": ["äº¤ä»˜åŸ¹è®­", "æ‚¬æŒ‚ä¸èˆ’é€‚åº¦"],
                    "é—®é¢˜ç±»å‹": [],
                    "æƒ…æ„Ÿ": "æ­£é¢"
                },
                {
                    "text": "å……ç”µæ¡©å……ç”µé€Ÿåº¦å¤ªæ…¢äº†ï¼Œè€Œä¸”ç»å¸¸å",
                    "æ—…ç¨‹è§¦ç‚¹": ["å…¬å…±å……ç”µç«™ä½“éªŒ"],
                    "é—®é¢˜ç±»å‹": ["æ€§èƒ½é—®é¢˜", "ç¨³å®šæ€§é—®é¢˜"],
                    "æƒ…æ„Ÿ": "è´Ÿé¢"
                },
                {
                    "text": "OTAå‡çº§åç³»ç»Ÿæ›´æµç•…äº†ï¼Œæ–°åŠŸèƒ½ä¹Ÿå¾ˆå®ç”¨",
                    "æ—…ç¨‹è§¦ç‚¹": ["OTAæ›´æ–°ä½“éªŒ"],
                    "é—®é¢˜ç±»å‹": [],
                    "æƒ…æ„Ÿ": "æ­£é¢"
                },
                {
                    "text": "é”€å”®é¡¾é—®å¾ˆä¸“ä¸šï¼Œä»·æ ¼ä¹Ÿå¾ˆé€æ˜ï¼Œæ²¡æœ‰ä¹±æ”¶è´¹",
                    "æ—…ç¨‹è§¦ç‚¹": ["é”€å”®æœåŠ¡æ€åº¦", "ä»·æ ¼é€æ˜åº¦"],
                    "é—®é¢˜ç±»å‹": [],
                    "æƒ…æ„Ÿ": "æ­£é¢"
                },
                {
                    "text": "è½¦æœºç³»ç»Ÿå¡é¡¿ï¼Œè§¦æ‘¸ä¸çµæ•ï¼Œå½±å“é©¾é©¶ä½“éªŒ",
                    "æ—…ç¨‹è§¦ç‚¹": ["HUD/ä¸­æ§äº¤äº’"],
                    "é—®é¢˜ç±»å‹": ["ç¨³å®šæ€§é—®é¢˜", "å¯ç”¨æ€§é—®é¢˜"],
                    "æƒ…æ„Ÿ": "è´Ÿé¢"
                },
                {
                    "text": "ç»­èˆªæ¯”å®˜æ–¹å®£ä¼ çš„è¦çŸ­ï¼Œå†¬å¤©æ‰ç”µç‰¹åˆ«å¿«",
                    "æ—…ç¨‹è§¦ç‚¹": ["ç»­èˆªè¡¨ç°"],
                    "é—®é¢˜ç±»å‹": ["æœŸå¾…è½å·®", "æ€§èƒ½é—®é¢˜"],
                    "æƒ…æ„Ÿ": "è´Ÿé¢"
                },
                {
                    "text": "å®¢æœå“åº”å¾ˆåŠæ—¶ï¼Œé—®é¢˜è§£å†³å¾—ä¹Ÿå¾ˆå¿«",
                    "æ—…ç¨‹è§¦ç‚¹": ["å®¢æœå“åº”é€Ÿåº¦"],
                    "é—®é¢˜ç±»å‹": [],
                    "æƒ…æ„Ÿ": "æ­£é¢"
                }
            ]
        else:
            # é»˜è®¤ç¤ºä¾‹æ•°æ®
            examples = [
                {
                    "text": "è¿™ä¸ªäº§å“çš„ä»·æ ¼å¤ªè´µäº†",
                    "journey_touchpoints": "è´­ä¹°æœŸ",
                    "problem_categories": "ä»·æ ¼é—®é¢˜",
                    "sentiment": "è´Ÿé¢"
                },
                {
                    "text": "å®¢æœå“åº”å¾ˆå¿«ï¼Œé—®é¢˜éƒ½è§£å†³äº†",
                    "journey_touchpoints": "æœåŠ¡æœŸ",
                    "problem_categories": "",
                    "sentiment": "æ­£é¢"
                },
                {
                    "text": "ç³»ç»Ÿç»å¸¸é—ªé€€ï¼Œå¾ˆå½±å“ä½¿ç”¨",
                    "journey_touchpoints": "ä½¿ç”¨æœŸ",
                    "problem_categories": "ç³»ç»Ÿæ•…éšœ",
                    "sentiment": "è´Ÿé¢"
                },
                {
                    "text": "åŒ…è£…å¾ˆç²¾ç¾ï¼Œè´¨æ„Ÿå¾ˆå¥½",
                    "journey_touchpoints": "è´­ä¹°æœŸ",
                    "problem_categories": "",
                    "sentiment": "æ­£é¢"
                }
            ]
        
        # æ¨¡æ‹ŸåŠ è½½è¿‡ç¨‹
        with tqdm(total=len(examples), desc="ç¤ºä¾‹æ•°æ®å‡†å¤‡", ncols=80) as pbar:
            for i in range(len(examples)):
                time.sleep(0.1)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
                pbar.update(1)
        
        # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
        texts = [ex["text"] for ex in examples]
        
        # æ„å»ºæ ‡ç­¾å­—å…¸
        labels_dict = {}
        if examples:
            # è·å–æ‰€æœ‰æ ‡ç­¾é”®ï¼ˆé™¤äº†textï¼‰
            all_keys = set()
            for ex in examples:
                all_keys.update(k for k in ex.keys() if k != "text")
            
            # åˆå§‹åŒ–æ ‡ç­¾å­—å…¸
            for key in all_keys:
                labels_dict[key] = []
            
            # å¡«å……æ ‡ç­¾å€¼
            for ex in examples:
                for key in all_keys:
                    value = ex.get(key, "")
                    if isinstance(value, list):
                        # å¤šæ ‡ç­¾è½¬æ¢ä¸ºé€—å·åˆ†éš”å­—ç¬¦ä¸²
                        labels_dict[key].append(",".join(value) if value else "")
                    else:
                        labels_dict[key].append(str(value) if value else "")
        
        print(f"âœ… ç¤ºä¾‹æ•°æ®åŠ è½½å®Œæˆ: {len(texts)}æ¡æ–‡æœ¬, {len(labels_dict)}ç§æ ‡ç­¾")
        
        # æ˜¾ç¤ºæ ‡ç­¾ç»Ÿè®¡
        for label_type, labels in labels_dict.items():
            non_empty = sum(1 for label in labels if label.strip())
            print(f"  {label_type}: {non_empty}/{len(labels)}æ¡æœ‰æ ‡ç­¾")
        
        return texts, labels_dict
    
    def batch_load_files(self, 
                        file_paths: List[str], 
                        text_column: str = "text") -> Tuple[List[str], Dict[str, List], pd.DataFrame]:
        """
        æ‰¹é‡åŠ è½½å¤šä¸ªæ–‡ä»¶
        
        Args:
            file_paths: æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            text_column: æ–‡æœ¬åˆ—å
            
        Returns:
            åˆå¹¶åçš„æ•°æ®
        """
        all_texts = []
        all_labels = {}
        all_dfs = []
        
        print(f"ğŸ“ æ‰¹é‡åŠ è½½ {len(file_paths)} ä¸ªæ–‡ä»¶...")
        
        for file_path in tqdm(file_paths, desc="æ‰¹é‡åŠ è½½", ncols=80):
            try:
                texts, labels_dict, df = self.load_file(file_path, text_column)
                
                all_texts.extend(texts)
                all_dfs.append(df)
                
                # åˆå¹¶æ ‡ç­¾
                for label_type, labels in labels_dict.items():
                    if label_type not in all_labels:
                        all_labels[label_type] = []
                    all_labels[label_type].extend(labels)
                
            except Exception as e:
                self.logger.error(f"åŠ è½½æ–‡ä»¶å¤±è´¥ {file_path}: {str(e)}")
                continue
        
        # åˆå¹¶DataFrame
        combined_df = pd.concat(all_dfs, ignore_index=True) if all_dfs else pd.DataFrame()
        
        print(f"âœ… æ‰¹é‡åŠ è½½å®Œæˆ: {len(all_texts)}æ¡æ–‡æœ¬")
        return all_texts, all_labels, combined_df
    
    def save_data(self, 
                  texts: List[str], 
                  labels_dict: Dict[str, List], 
                  output_path: str,
                  text_column: str = "text"):
        """
        ä¿å­˜æ•°æ®åˆ°æ–‡ä»¶
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            labels_dict: æ ‡ç­¾å­—å…¸
            output_path: è¾“å‡ºè·¯å¾„
            text_column: æ–‡æœ¬åˆ—å
        """
        try:
            # æ„å»ºDataFrame
            data = {text_column: texts}
            data.update(labels_dict)
            
            df = pd.DataFrame(data)
            
            # æ ¹æ®æ–‡ä»¶æ‰©å±•åé€‰æ‹©ä¿å­˜æ ¼å¼
            file_ext = Path(output_path).suffix.lower()
            
            if file_ext == '.csv':
                df.to_csv(output_path, index=False, encoding='utf-8')
            elif file_ext == '.json':
                df.to_json(output_path, orient='records', ensure_ascii=False, indent=2)
            elif file_ext in ['.xlsx', '.xls']:
                df.to_excel(output_path, index=False)
            else:
                # é»˜è®¤ä¿å­˜ä¸ºCSV
                df.to_csv(output_path, index=False, encoding='utf-8')
            
            print(f"âœ… æ•°æ®å·²ä¿å­˜åˆ°: {output_path}")
            
        except Exception as e:
            print(f"âŒ ä¿å­˜æ•°æ®å¤±è´¥: {str(e)}")
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–åŠ è½½ç»Ÿè®¡ä¿¡æ¯"""
        avg_load_time = np.mean(self.stats['load_times']) if self.stats['load_times'] else 0
        
        return {
            'loaded_files': self.stats['loaded_files'],
            'total_records': self.stats['total_records'],
            'valid_records': self.stats['valid_records'],
            'invalid_records': self.stats['invalid_records'],
            'avg_load_time': avg_load_time,
            'encoding_detections': self.stats['encoding_detections']
        }
    
    def print_stats(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.get_stats()
        
        print(f"\nğŸ“Š æ•°æ®åŠ è½½ç»Ÿè®¡:")
        print(f"  å·²åŠ è½½æ–‡ä»¶: {stats['loaded_files']}")
        print(f"  æ€»è®°å½•æ•°: {stats['total_records']}")
        print(f"  æœ‰æ•ˆè®°å½•: {stats['valid_records']}")
        print(f"  æ— æ•ˆè®°å½•: {stats['invalid_records']}")
        print(f"  å¹³å‡åŠ è½½æ—¶é—´: {stats['avg_load_time']:.2f}ç§’")
        
        if stats['encoding_detections']:
            print(f"\nğŸ”¤ ç¼–ç æ£€æµ‹:")
            for file_path, detection in stats['encoding_detections'].items():
                file_name = os.path.basename(file_path)
                print(f"  {file_name}: {detection['encoding']} ({detection['confidence']:.2f})")
    
    def validate_voc_labels(self, labels_dict: Dict[str, List]) -> Dict[str, Any]:
        """
        éªŒè¯VOCæ ‡ç­¾æ ¼å¼
        
        Args:
            labels_dict: æ ‡ç­¾å­—å…¸
            
        Returns:
            éªŒè¯ç»“æœ
        """
        validation_results = {}
        
        for label_type, labels in labels_dict.items():
            if label_type not in self.voc_label_schema:
                validation_results[label_type] = {
                    'valid': False,
                    'error': f"æœªçŸ¥æ ‡ç­¾ç±»å‹: {label_type}"
                }
                continue
            
            schema = self.voc_label_schema[label_type]
            expected_values = set(schema['values'])
            
            # æ”¶é›†æ‰€æœ‰å®é™…æ ‡ç­¾å€¼
            actual_values = set()
            for label in labels:
                if label.strip():
                    if schema['type'] == 'multilabel':
                        # åˆ†å‰²å¤šæ ‡ç­¾
                        label_list = [l.strip() for l in label.split(',') if l.strip()]
                        actual_values.update(label_list)
                    else:
                        actual_values.add(label.strip())
            
            # æ£€æŸ¥æ— æ•ˆæ ‡ç­¾
            invalid_labels = actual_values - expected_values
            
            validation_results[label_type] = {
                'valid': len(invalid_labels) == 0,
                'invalid_labels': list(invalid_labels),
                'valid_labels': list(actual_values & expected_values),
                'coverage': len(actual_values & expected_values) / len(expected_values) * 100
            }
        
        return validation_results


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    loader = DataLoader(enable_cache=True)
    
    # åŠ è½½ç¤ºä¾‹æ•°æ®
    texts, labels_dict = loader.load_examples("voc")
    
    # éªŒè¯VOCæ ‡ç­¾
    validation = loader.validate_voc_labels(labels_dict)
    print(f"\nğŸ” æ ‡ç­¾éªŒè¯ç»“æœ:")
    for label_type, result in validation.items():
        status = "âœ…" if result['valid'] else "âŒ"
        print(f"  {status} {label_type}: {result}")
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    loader.print_stats()