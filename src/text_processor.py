"""
æ–‡æœ¬é¢„å¤„ç†å™¨ - æ”¹è¿›ç‰ˆ
ä¸“ä¸ºVOCæ–‡æœ¬åˆ†æä¼˜åŒ–ï¼Œæ”¯æŒä¸­æ–‡åˆ†è¯ã€æ¸…æ´—ã€æ ‡å‡†åŒ–ç­‰åŠŸèƒ½
"""
import re
import string
import unicodedata
from typing import List, Dict, Optional, Union
import logging
from tqdm import tqdm
import jieba
import jieba.posseg as pseg
from collections import Counter
import pandas as pd

# é…ç½®jiebaæ—¥å¿—çº§åˆ«ï¼Œé¿å…è¿‡å¤šè¾“å‡º
jieba.setLogLevel(logging.INFO)

class TextProcessor:
    """
    ä¸“ä¸ºVOCåˆ†æè®¾è®¡çš„æ–‡æœ¬é¢„å¤„ç†å™¨
    æ”¯æŒä¸­æ–‡åˆ†è¯ã€åœç”¨è¯è¿‡æ»¤ã€æ–‡æœ¬æ ‡å‡†åŒ–ç­‰åŠŸèƒ½
    """
    
    def __init__(self, 
                 custom_dict_path: Optional[str] = None,
                 stopwords_path: Optional[str] = None,
                 enable_userdict: bool = True):
        """
        åˆå§‹åŒ–æ–‡æœ¬å¤„ç†å™¨
        
        Args:
            custom_dict_path: è‡ªå®šä¹‰è¯å…¸è·¯å¾„
            stopwords_path: åœç”¨è¯æ–‡ä»¶è·¯å¾„
            enable_userdict: æ˜¯å¦å¯ç”¨ç”¨æˆ·è¯å…¸
        """
        self.logger = logging.getLogger(__name__)
        
        # åˆå§‹åŒ–jiebaåˆ†è¯å™¨
        self._init_jieba(custom_dict_path, enable_userdict)
        
        # åŠ è½½åœç”¨è¯
        self.stopwords = self._load_stopwords(stopwords_path)
        
        # å®šä¹‰æ¸…æ´—è§„åˆ™
        self._init_cleaning_rules()
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'processed_count': 0,
            'avg_length_before': 0,
            'avg_length_after': 0,
            'empty_texts': 0
        }
    
    def _init_jieba(self, custom_dict_path: Optional[str], enable_userdict: bool):
        """åˆå§‹åŒ–jiebaåˆ†è¯å™¨"""
        try:
            # åŠ è½½è‡ªå®šä¹‰è¯å…¸
            if custom_dict_path and enable_userdict:
                jieba.load_userdict(custom_dict_path)
                self.logger.info(f"å·²åŠ è½½è‡ªå®šä¹‰è¯å…¸: {custom_dict_path}")
            
            # æ·»åŠ VOCç›¸å…³ä¸“ä¸šè¯æ±‡
            if enable_userdict:
                self._add_voc_vocabulary()
                
        except Exception as e:
            self.logger.warning(f"åˆå§‹åŒ–jiebaå¤±è´¥: {str(e)}")
    
    def _add_voc_vocabulary(self):
        """æ·»åŠ VOCå’Œæ±½è½¦è¡Œä¸šç›¸å…³è¯æ±‡"""
        voc_words = [
            # æ±½è½¦ç›¸å…³
            'ç†æƒ³æ±½è½¦', 'ç†æƒ³ONE', 'ç†æƒ³L9', 'ç†æƒ³L8', 'ç†æƒ³L7',
            'å¢ç¨‹å¼', 'çº¯ç”µåŠ¨', 'æ··åˆåŠ¨åŠ›', 'NEDC', 'WLTP',
            'è‡ªåŠ¨é©¾é©¶', 'è¾…åŠ©é©¾é©¶', 'NOA', 'AEB', 'ACC',
            
            # ç”¨æˆ·ä½“éªŒç›¸å…³
            'ç”¨æˆ·ä½“éªŒ', 'äº¤äº’ä½“éªŒ', 'è¯­éŸ³åŠ©æ‰‹', 'è½¦æœºç³»ç»Ÿ',
            'æ™ºèƒ½åº§èˆ±', 'æ™ºèƒ½ç½‘è”', 'OTAå‡çº§', 'è¿œç¨‹æ§åˆ¶',
            
            # é—®é¢˜ç±»å‹ç›¸å…³
            'ç»­èˆªé‡Œç¨‹', 'å……ç”µé€Ÿåº¦', 'å……ç”µæ¡©', 'èƒ½è€—è¡¨ç°',
            'é©¾é©¶ä½“éªŒ', 'ä¹˜åä½“éªŒ', 'éš”éŸ³æ•ˆæœ', 'æ‚¬æŒ‚è°ƒæ ¡',
            
            # æœåŠ¡ç›¸å…³
            'é”€å”®é¡¾é—®', 'äº¤ä»˜ä¸­å¿ƒ', 'å”®åæœåŠ¡', 'å®¢æœçƒ­çº¿',
            'ä¿å…»ç»´ä¿®', 'é›¶é…ä»¶', 'è´¨ä¿æœŸ', 'å¬å›é€šçŸ¥'
        ]
        
        for word in voc_words:
            jieba.add_word(word, freq=1000)  # è®¾ç½®è¾ƒé«˜é¢‘ç‡ç¡®ä¿æ­£ç¡®åˆ†è¯
    
    def _load_stopwords(self, stopwords_path: Optional[str]) -> set:
        """åŠ è½½åœç”¨è¯è¡¨"""
        stopwords = set()
        
        # é»˜è®¤åœç”¨è¯
        default_stopwords = {
            # æ ‡ç‚¹ç¬¦å·
            'ã€‚', 'ï¼Œ', 'ã€', 'ï¼›', 'ï¼š', 'ï¼Ÿ', 'ï¼', '"', '"', ''', ''',
            'ï¼ˆ', 'ï¼‰', 'ã€', 'ã€‘', 'ã€Š', 'ã€‹', 'Â·', 'â€¦', 'â€”', 'ï¼',
            '.', ',', ';', ':', '?', '!', '"', "'", '(', ')', '[', ']',
            '{', '}', '<', '>', '/', '\\', '|', '-', '_', '+', '=',
            '*', '&', '^', '%', '$', '#', '@', '~', '`',
            
            # å¸¸è§åœç”¨è¯
            'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº',
            'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»',
            'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™', 'é‚£',
            'ä½†æ˜¯', 'ç„¶å', 'å› ä¸º', 'æ‰€ä»¥', 'å¦‚æœ', 'è™½ç„¶', 'æˆ–è€…',
            
            # ç½‘ç»œç”¨è¯­
            'å“ˆå“ˆ', 'å‘µå‘µ', 'å—¯', 'å•Š', 'å“¦', 'é¢', 'å—¯å—¯', 'å“',
            
            # æ— æ„ä¹‰è¯æ±‡
            'ä¸œè¥¿', 'ä»€ä¹ˆ', 'æ€ä¹ˆ', 'è¿™æ ·', 'é‚£æ ·', 'è¿™ç§', 'é‚£ç§'
        }
        
        stopwords.update(default_stopwords)
        
        # ä»æ–‡ä»¶åŠ è½½åœç”¨è¯
        if stopwords_path:
            try:
                with open(stopwords_path, 'r', encoding='utf-8') as f:
                    file_stopwords = {line.strip() for line in f if line.strip()}
                    stopwords.update(file_stopwords)
                self.logger.info(f"å·²åŠ è½½åœç”¨è¯æ–‡ä»¶: {stopwords_path}, å…±{len(file_stopwords)}ä¸ª")
            except Exception as e:
                self.logger.warning(f"åŠ è½½åœç”¨è¯æ–‡ä»¶å¤±è´¥: {str(e)}")
        
        self.logger.info(f"åœç”¨è¯æ€»æ•°: {len(stopwords)}")
        return stopwords
    
    def _init_cleaning_rules(self):
        """åˆå§‹åŒ–æ–‡æœ¬æ¸…æ´—è§„åˆ™"""
        # ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼ä»¥æé«˜æ€§èƒ½
        self.patterns = {
            # ç§»é™¤URL
            'url': re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'),
            
            # ç§»é™¤é‚®ç®±
            'email': re.compile(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'),
            
            # ç§»é™¤æ‰‹æœºå·
            'phone': re.compile(r'1[3-9]\d{9}'),
            
            # ç§»é™¤å¤šä½™ç©ºç™½å­—ç¬¦
            'whitespace': re.compile(r'\s+'),
            
            # ç§»é™¤é‡å¤æ ‡ç‚¹
            'repeat_punct': re.compile(r'([ã€‚ï¼ï¼Ÿï¼Œã€ï¼›ï¼š])\1+'),
            
            # ç§»é™¤HTMLæ ‡ç­¾
            'html': re.compile(r'<[^>]+>'),
            
            # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼ˆä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—ã€åŸºæœ¬æ ‡ç‚¹ï¼‰
            'special_chars': re.compile(r'[^\u4e00-\u9fa5a-zA-Z0-9ã€‚ï¼Œï¼ï¼Ÿã€ï¼›ï¼š""''ï¼ˆï¼‰ã€ã€‘ã€Šã€‹\s]'),
            
            # ç§»é™¤emoji
            'emoji': re.compile(r'[\U0001F600-\U0001F64F\U0001F300-\U0001F5FF\U0001F680-\U0001F6FF\U0001F1E0-\U0001F1FF]+'),
            
            # åŒ¹é…çº¯æ•°å­—
            'pure_number': re.compile(r'^\d+$'),
            
            # åŒ¹é…å•ä¸ªå­—ç¬¦
            'single_char': re.compile(r'^.$')
        }
    
    def clean_text(self, text: str) -> str:
        """
        æ¸…æ´—å•ä¸ªæ–‡æœ¬
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            æ¸…æ´—åçš„æ–‡æœ¬
        """
        if not isinstance(text, str):
            return ""
        
        original_text = text
        
        # 1. è½¬æ¢ä¸ºå°å†™ï¼ˆè‹±æ–‡éƒ¨åˆ†ï¼‰
        # text = text.lower()  # æ³¨é‡Šæ‰ï¼Œä¿æŒåŸå§‹å¤§å°å†™ä»¥ä¿ç•™è¯­ä¹‰ä¿¡æ¯
        
        # 2. Unicodeæ ‡å‡†åŒ–
        text = unicodedata.normalize('NFKC', text)
        
        # 3. ç§»é™¤URL
        text = self.patterns['url'].sub('', text)
        
        # 4. ç§»é™¤é‚®ç®±
        text = self.patterns['email'].sub('', text)
        
        # 5. ç§»é™¤æ‰‹æœºå·
        text = self.patterns['phone'].sub('', text)
        
        # 6. ç§»é™¤HTMLæ ‡ç­¾
        text = self.patterns['html'].sub('', text)
        
        # 7. ç§»é™¤emoji
        text = self.patterns['emoji'].sub('', text)
        
        # 8. ç§»é™¤ç‰¹æ®Šå­—ç¬¦
        text = self.patterns['special_chars'].sub('', text)
        
        # 9. å¤„ç†é‡å¤æ ‡ç‚¹
        text = self.patterns['repeat_punct'].sub(r'\1', text)
        
        # 10. ç»Ÿä¸€ç©ºç™½å­—ç¬¦
        text = self.patterns['whitespace'].sub(' ', text)
        
        # 11. å»é™¤é¦–å°¾ç©ºæ ¼
        text = text.strip()
        
        return text
    
    def segment_text(self, text: str, pos_filter: Optional[List[str]] = None) -> List[str]:
        """
        å¯¹æ–‡æœ¬è¿›è¡Œåˆ†è¯
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            pos_filter: è¯æ€§è¿‡æ»¤åˆ—è¡¨ï¼Œå¦‚['n', 'v', 'a']è¡¨ç¤ºåªä¿ç•™åè¯ã€åŠ¨è¯ã€å½¢å®¹è¯
            
        Returns:
            åˆ†è¯ç»“æœåˆ—è¡¨
        """
        if not text:
            return []
        
        try:
            if pos_filter:
                # å¸¦è¯æ€§æ ‡æ³¨çš„åˆ†è¯
                words = []
                for word, pos in pseg.cut(text):
                    if (pos[0] in pos_filter and  # è¯æ€§è¿‡æ»¤
                        word not in self.stopwords and  # åœç”¨è¯è¿‡æ»¤
                        len(word.strip()) > 1 and  # é•¿åº¦è¿‡æ»¤
                        not self.patterns['pure_number'].match(word) and  # çº¯æ•°å­—è¿‡æ»¤
                        not self.patterns['single_char'].match(word)):  # å•å­—ç¬¦è¿‡æ»¤
                        words.append(word.strip())
            else:
                # æ™®é€šåˆ†è¯
                words = []
                for word in jieba.cut(text):
                    if (word not in self.stopwords and
                        len(word.strip()) > 1 and
                        not self.patterns['pure_number'].match(word) and
                        not self.patterns['single_char'].match(word)):
                        words.append(word.strip())
            
            return words
            
        except Exception as e:
            self.logger.warning(f"åˆ†è¯å¤±è´¥: {str(e)}")
            return text.split()
    
    def preprocess(self, text: str, 
                   enable_segmentation: bool = True,
                   pos_filter: Optional[List[str]] = None,
                   min_length: int = 2,
                   max_length: int = 500) -> str:
        """
        å®Œæ•´çš„æ–‡æœ¬é¢„å¤„ç†
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            enable_segmentation: æ˜¯å¦å¯ç”¨åˆ†è¯
            pos_filter: è¯æ€§è¿‡æ»¤
            min_length: æœ€å°æ–‡æœ¬é•¿åº¦
            max_length: æœ€å¤§æ–‡æœ¬é•¿åº¦
            
        Returns:
            é¢„å¤„ç†åçš„æ–‡æœ¬
        """
        if not isinstance(text, str):
            return ""
        
        original_length = len(text)
        
        # 1. åŸºç¡€æ¸…æ´—
        cleaned_text = self.clean_text(text)
        
        # 2. é•¿åº¦æ£€æŸ¥
        if len(cleaned_text) < min_length:
            self.stats['empty_texts'] += 1
            return ""
        
        # 3. æˆªæ–­è¿‡é•¿æ–‡æœ¬
        if len(cleaned_text) > max_length:
            cleaned_text = cleaned_text[:max_length]
        
        # 4. åˆ†è¯å¤„ç†
        if enable_segmentation:
            words = self.segment_text(cleaned_text, pos_filter)
            processed_text = ' '.join(words) if words else ""
        else:
            processed_text = cleaned_text
        
        # 5. æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        self.stats['processed_count'] += 1
        self.stats['avg_length_before'] = (
            (self.stats['avg_length_before'] * (self.stats['processed_count'] - 1) + original_length) /
            self.stats['processed_count']
        )
        self.stats['avg_length_after'] = (
            (self.stats['avg_length_after'] * (self.stats['processed_count'] - 1) + len(processed_text)) /
            self.stats['processed_count']
        )
        
        return processed_text
    
    def batch_process(self, 
                     texts: List[str], 
                     enable_segmentation: bool = True,
                     pos_filter: Optional[List[str]] = None,
                     min_length: int = 2,
                     max_length: int = 500,
                     show_progress: bool = True) -> List[str]:
        """
        æ‰¹é‡å¤„ç†æ–‡æœ¬
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            enable_segmentation: æ˜¯å¦å¯ç”¨åˆ†è¯
            pos_filter: è¯æ€§è¿‡æ»¤
            min_length: æœ€å°æ–‡æœ¬é•¿åº¦
            max_length: æœ€å¤§æ–‡æœ¬é•¿åº¦
            show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦æ¡
            
        Returns:
            å¤„ç†åçš„æ–‡æœ¬åˆ—è¡¨
        """
        if not texts:
            return []
        
        # é‡ç½®ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'processed_count': 0,
            'avg_length_before': 0,
            'avg_length_after': 0,
            'empty_texts': 0
        }
        
        processed_texts = []
        
        # åˆ›å»ºè¿›åº¦æ¡
        iterator = tqdm(texts, desc="æ–‡æœ¬é¢„å¤„ç†", disable=not show_progress, ncols=80)
        
        for text in iterator:
            processed_text = self.preprocess(
                text, 
                enable_segmentation=enable_segmentation,
                pos_filter=pos_filter,
                min_length=min_length,
                max_length=max_length
            )
            processed_texts.append(processed_text)
            
            # æ›´æ–°è¿›åº¦æ¡æè¿°
            if show_progress and self.stats['processed_count'] % 100 == 0:
                iterator.set_postfix({
                    'æœ‰æ•ˆ': f"{self.stats['processed_count'] - self.stats['empty_texts']}/{self.stats['processed_count']}",
                    'å¹³å‡é•¿åº¦': f"{self.stats['avg_length_after']:.1f}"
                })
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        self._print_stats()
        
        return processed_texts
    
    def extract_keywords(self, 
                        texts: List[str], 
                        top_k: int = 20,
                        min_freq: int = 2) -> Dict[str, int]:
        """
        ä»æ–‡æœ¬é›†åˆä¸­æå–å…³é”®è¯
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            top_k: è¿”å›å‰kä¸ªå…³é”®è¯
            min_freq: æœ€å°è¯é¢‘
            
        Returns:
            å…³é”®è¯åŠå…¶é¢‘ç‡çš„å­—å…¸
        """
        all_words = []
        
        for text in tqdm(texts, desc="æå–å…³é”®è¯", ncols=80):
            words = self.segment_text(text, pos_filter=['n', 'v', 'a'])  # åªä¿ç•™åè¯ã€åŠ¨è¯ã€å½¢å®¹è¯
            all_words.extend(words)
        
        # ç»Ÿè®¡è¯é¢‘
        word_freq = Counter(all_words)
        
        # è¿‡æ»¤ä½é¢‘è¯
        filtered_freq = {word: freq for word, freq in word_freq.items() if freq >= min_freq}
        
        # è¿”å›top_kä¸ªå…³é”®è¯
        return dict(Counter(filtered_freq).most_common(top_k))
    
    def analyze_text_quality(self, texts: List[str]) -> Dict[str, Union[int, float]]:
        """
        åˆ†ææ–‡æœ¬è´¨é‡
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            
        Returns:
            æ–‡æœ¬è´¨é‡åˆ†æç»“æœ
        """
        if not texts:
            return {}
        
        # ç»Ÿè®¡æŒ‡æ ‡
        total_texts = len(texts)
        non_empty_texts = sum(1 for text in texts if text.strip())
        total_chars = sum(len(text) for text in texts)
        total_words = sum(len(self.segment_text(text)) for text in texts if text.strip())
        
        # é•¿åº¦åˆ†å¸ƒ
        lengths = [len(text) for text in texts]
        
        return {
            'total_texts': total_texts,
            'non_empty_texts': non_empty_texts,
            'empty_rate': (total_texts - non_empty_texts) / total_texts * 100,
            'avg_chars': total_chars / total_texts if total_texts > 0 else 0,
            'avg_words': total_words / non_empty_texts if non_empty_texts > 0 else 0,
            'min_length': min(lengths) if lengths else 0,
            'max_length': max(lengths) if lengths else 0,
            'median_length': sorted(lengths)[len(lengths)//2] if lengths else 0
        }
    
    def _print_stats(self):
        """æ‰“å°å¤„ç†ç»Ÿè®¡ä¿¡æ¯"""
        if self.stats['processed_count'] > 0:
            print(f"\nğŸ“Š æ–‡æœ¬å¤„ç†ç»Ÿè®¡:")
            print(f"  æ€»å¤„ç†æ•°é‡: {self.stats['processed_count']}")
            print(f"  æœ‰æ•ˆæ–‡æœ¬: {self.stats['processed_count'] - self.stats['empty_texts']}")
            print(f"  ç©ºæ–‡æœ¬æ•°é‡: {self.stats['empty_texts']}")
            print(f"  å¹³å‡é•¿åº¦(å¤„ç†å‰): {self.stats['avg_length_before']:.1f}")
            print(f"  å¹³å‡é•¿åº¦(å¤„ç†å): {self.stats['avg_length_after']:.1f}")
            print(f"  å¤„ç†æ•ˆç‡: {((self.stats['processed_count'] - self.stats['empty_texts']) / self.stats['processed_count'] * 100):.1f}%")
    
    def get_stats(self) -> Dict[str, Union[int, float]]:
        """è·å–å¤„ç†ç»Ÿè®¡ä¿¡æ¯"""
        return self.stats.copy()
    
    def reset_stats(self):
        """é‡ç½®ç»Ÿè®¡ä¿¡æ¯"""
        self.stats = {
            'processed_count': 0,
            'avg_length_before': 0,
            'avg_length_after': 0,
            'empty_texts': 0
        }
    
    def save_processed_data(self, 
                           original_texts: List[str], 
                           processed_texts: List[str], 
                           output_path: str):
        """
        ä¿å­˜å¤„ç†ç»“æœåˆ°æ–‡ä»¶
        
        Args:
            original_texts: åŸå§‹æ–‡æœ¬åˆ—è¡¨
            processed_texts: å¤„ç†åæ–‡æœ¬åˆ—è¡¨
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        try:
            df = pd.DataFrame({
                'original_text': original_texts,
                'processed_text': processed_texts,
                'original_length': [len(text) for text in original_texts],
                'processed_length': [len(text) for text in processed_texts]
            })
            
            df.to_csv(output_path, index=False, encoding='utf-8')
            print(f"âœ… å¤„ç†ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
            
        except Exception as e:
            print(f"âŒ ä¿å­˜å¤±è´¥: {str(e)}")


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºå¤„ç†å™¨å®ä¾‹
    processor = TextProcessor(enable_userdict=True)
    
    # ç¤ºä¾‹æ–‡æœ¬
    test_texts = [
        "ç†æƒ³æ±½è½¦çš„è¯­éŸ³åŠ©æ‰‹çœŸçš„å¾ˆæ™ºèƒ½ï¼ï¼ï¼ä½†æ˜¯æœ‰æ—¶å€™è¯†åˆ«ä¸å‡†ç¡®ğŸ˜…",
        "å……ç”µé€Ÿåº¦è¿˜å¯ä»¥ï¼Œä½†æ˜¯å……ç”µæ¡©å¤ªå°‘äº†ï¼Œå¸Œæœ›èƒ½å¤šå»ºä¸€äº›ã€‚",
        "é©¾é©¶ä½“éªŒéå¸¸å¥½ï¼Œåº§æ¤…å¾ˆèˆ’é€‚ï¼Œéš”éŸ³æ•ˆæœä¹Ÿä¸é”™ğŸ‘ğŸ‘ğŸ‘",
        "OTAå‡çº§åç³»ç»Ÿæ›´æµç•…äº†ï¼Œä½†æ˜¯å¶å°”è¿˜æ˜¯ä¼šå¡é¡¿ã€‚",
        ""  # ç©ºæ–‡æœ¬æµ‹è¯•
    ]
    
    # æ‰¹é‡å¤„ç†
    processed_texts = processor.batch_process(test_texts)
    
    # æ˜¾ç¤ºç»“æœ
    print("\nğŸ” å¤„ç†ç»“æœå¯¹æ¯”:")
    for i, (original, processed) in enumerate(zip(test_texts, processed_texts)):
        if original:  # è·³è¿‡ç©ºæ–‡æœ¬
            print(f"\n{i+1}. åŸæ–‡: {original}")
            print(f"   å¤„ç†å: {processed}")
    
    # æå–å…³é”®è¯
    keywords = processor.extract_keywords(processed_texts, top_k=10)
    print(f"\nğŸ”‘ å…³é”®è¯æå–: {keywords}")
    
    # åˆ†ææ–‡æœ¬è´¨é‡
    quality = processor.analyze_text_quality(test_texts)
    print(f"\nğŸ“ˆ æ–‡æœ¬è´¨é‡åˆ†æ: {quality}")